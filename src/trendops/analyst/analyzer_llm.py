# src/trendops/analyst/analyzer_llm.py
"""
vLLM ê¸°ë°˜ ë‰´ìŠ¤ ë¶„ì„ í´ë¼ì´ì–¸íŠ¸

Blueprint Week 2: LLM ì—°ë™
- AsyncOpenAI í´ë¼ì´ì–¸íŠ¸ ì‚¬ìš© (vLLM OpenAI API í˜¸í™˜)
- ì¤‘ë¦½ì  ë‰´ìŠ¤ ë¶„ì„ê°€ í˜ë¥´ì†Œë‚˜
- JSON êµ¬ì¡°í™” ì¶œë ¥ (Week 4ì—ì„œ Outlinesë¡œ 100% ë³´ì¥ ì˜ˆì •)
"""

from __future__ import annotations

import asyncio
import json
import re
from datetime import datetime
from typing import Any

from openai import AsyncOpenAI, APIError, APIConnectionError, RateLimitError
from pydantic import BaseModel, Field, field_validator

from trendops.config.settings import get_settings
from trendops.utils.logger import get_logger

logger = get_logger(__name__)


# =============================================================================
# Pydantic Models (Blueprint Section 3: Analysis Results Schema)
# =============================================================================

class SentimentRatio(BaseModel):
    """ê°ì„± ë¹„ìœ¨ ìŠ¤í‚¤ë§ˆ"""
    positive: float = Field(..., ge=0.0, le=1.0, description="ê¸ì • ë¹„ìœ¨")
    negative: float = Field(..., ge=0.0, le=1.0, description="ë¶€ì • ë¹„ìœ¨")
    neutral: float = Field(..., ge=0.0, le=1.0, description="ì¤‘ë¦½ ë¹„ìœ¨")
    
    @field_validator("positive", "negative", "neutral", mode="after")
    @classmethod
    def round_ratio(cls, v: float) -> float:
        return round(v, 2)
    
    def model_post_init(self, __context: Any) -> None:
        """ë¹„ìœ¨ í•©ì´ 1.0ì´ ë˜ë„ë¡ ì •ê·œí™”"""
        total = self.positive + self.negative + self.neutral
        if total > 0 and abs(total - 1.0) > 0.01:
            self.positive = round(self.positive / total, 2)
            self.negative = round(self.negative / total, 2)
            self.neutral = round(1.0 - self.positive - self.negative, 2)


class AnalysisOutput(BaseModel):
    """
    LLM ë¶„ì„ ì¶œë ¥ ìŠ¤í‚¤ë§ˆ
    
    Blueprint Section 6.1 ì°¸ì¡°:
    Week 4ì—ì„œ Outlines/guided_decodingìœ¼ë¡œ 100% JSON ë³´ì¥ ì˜ˆì •
    """
    main_cause: str = Field(
        ..., 
        min_length=10,
        max_length=200,
        description="ì´ í‚¤ì›Œë“œê°€ ëœ¬ í•µì‹¬ ì›ì¸ (1ë¬¸ì¥)"
    )
    sentiment_ratio: SentimentRatio = Field(
        ...,
        description="ì—¬ë¡  ê°ì„± ë¹„ìœ¨"
    )
    key_opinions: list[str] = Field(
        ...,
        min_length=3,
        max_length=5,
        description="í•µì‹¬ ì˜ê²¬ 3-5ê°œ"
    )
    summary: str = Field(
        ...,
        min_length=50,
        max_length=300,
        description="3ì¤„ ìš”ì•½"
    )
    
    class Config:
        json_schema_extra = {
            "example": {
                "main_cause": "íŠ¸ëŸ¼í”„ ëŒ€í†µë ¹ì˜ ì¤‘êµ­ì‚° ì œí’ˆ 25% ê´€ì„¸ ë¶€ê³¼ ë°œí‘œë¡œ ì¸í•œ ê´€ì‹¬ ê¸‰ì¦",
                "sentiment_ratio": {
                    "positive": 0.15,
                    "negative": 0.55,
                    "neutral": 0.30
                },
                "key_opinions": [
                    "êµ­ë‚´ ìˆ˜ì¶œ ê¸°ì—…ë“¤ì˜ í”¼í•´ ìš°ë ¤ í™•ì‚°",
                    "ë°˜ë„ì²´Â·ë°°í„°ë¦¬ ì—…ì¢… ì£¼ê°€ í•˜ë½",
                    "ì†Œë¹„ì ë¬¼ê°€ ìƒìŠ¹ ì „ë§ì— ëŒ€í•œ ë¶ˆì•ˆê°"
                ],
                "summary": "íŠ¸ëŸ¼í”„ ëŒ€í†µë ¹ì´ ì¤‘êµ­ì‚° ì œí’ˆì— 25% ê´€ì„¸ë¥¼ ë¶€ê³¼í•œë‹¤ê³  ë°œí‘œí–ˆìŠµë‹ˆë‹¤.\nì´ì— ë”°ë¼ êµ­ë‚´ ìˆ˜ì¶œ ê¸°ì—…ë“¤ì˜ í”¼í•´ ìš°ë ¤ê°€ í™•ì‚°ë˜ê³  ìˆìŠµë‹ˆë‹¤.\níŠ¹íˆ ë°˜ë„ì²´ì™€ ë°°í„°ë¦¬ ì—…ì¢…ì˜ ì£¼ê°€ê°€ í•˜ë½í•˜ë©° ì‹œì¥ì´ ë¶ˆì•ˆí•´í•˜ê³  ìˆìŠµë‹ˆë‹¤."
            }
        }


class AnalysisResult(BaseModel):
    """ë¶„ì„ ê²°ê³¼ ì „ì²´ ìŠ¤í‚¤ë§ˆ"""
    keyword: str = Field(..., description="ë¶„ì„ ëŒ€ìƒ í‚¤ì›Œë“œ")
    analysis: AnalysisOutput = Field(..., description="LLM ë¶„ì„ ê²°ê³¼")
    source_count: int = Field(..., ge=0, description="ë¶„ì„ì— ì‚¬ìš©ëœ ì†ŒìŠ¤ ìˆ˜")
    model_version: str = Field(..., description="ì‚¬ìš©ëœ ëª¨ë¸ ë²„ì „")
    inference_time_seconds: float = Field(..., ge=0, description="ì¶”ë¡  ì†Œìš” ì‹œê°„")
    created_at: datetime = Field(default_factory=datetime.now, description="ìƒì„± ì‹œê°„")
    
    def is_valid(self) -> bool:
        """ë¶„ì„ ê²°ê³¼ ìœ íš¨ì„± ê²€ì‚¬"""
        return (
            len(self.analysis.main_cause) >= 10
            and len(self.analysis.key_opinions) >= 3
            and len(self.analysis.summary) >= 50
        )


# =============================================================================
# System Prompts (Blueprint Section 1.2.3 & 1.3.3)
# =============================================================================

# Blueprint Section 1.2.3: CONTENT_POLICY
CONTENT_POLICY = """
## í•„ìˆ˜ ì¤€ìˆ˜ ì‚¬í•­:
1. íŠ¹ì • ì •ì¹˜ì¸/ì •ë‹¹ì„ "ì¢‹ë‹¤/ë‚˜ì˜ë‹¤"ë¡œ í‰ê°€í•˜ì§€ ë§ˆì„¸ìš”.
2. ì›ë¬¸ ê¸°ì‚¬ë¥¼ ê·¸ëŒ€ë¡œ ì¸ìš©í•˜ì§€ ë§ê³ , í†µê³„ì  ìš”ì•½ë§Œ ì œê³µí•˜ì„¸ìš”.
3. "~í•œ ê²ƒìœ¼ë¡œ ì•Œë ¤ì¡Œë‹¤", "~ë¼ëŠ” ì˜ê²¬ì´ ìˆë‹¤" í˜•íƒœì˜ ê°ê´€ì  ì„œìˆ ë§Œ ì‚¬ìš©í•˜ì„¸ìš”.
4. ê²€ì¦ë˜ì§€ ì•Šì€ ì‚¬ì‹¤ì„ ë‹¨ì •ì ìœ¼ë¡œ ì„œìˆ í•˜ì§€ ë§ˆì„¸ìš”.
5. ê°œì¸ì„ íŠ¹ì •í•  ìˆ˜ ìˆëŠ” ì •ë³´(ì´ë¦„, ì—°ë½ì²˜ ë“±)ëŠ” ì ˆëŒ€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.

## ì½˜í…ì¸  ì„±ê²© ì •ì˜:
- ì´ ì½˜í…ì¸ ëŠ” "ì •ë³´ ìš”ì•½í˜•"ì…ë‹ˆë‹¤.
- ìš°ë¦¬ì˜ ì—­í• ì€ "í˜„ìƒ ì„¤ëª…"ì´ì§€, "ì˜ê²¬ ì œì‹œ"ê°€ ì•„ë‹™ë‹ˆë‹¤.
- ë…ìê°€ ìŠ¤ìŠ¤ë¡œ íŒë‹¨í•  ìˆ˜ ìˆë„ë¡ ê· í˜• ì¡íŒ ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤.
"""

# Blueprint Section 1.3.3: CONTENT_TONE_GUIDE
CONTENT_TONE_GUIDE = """
## ì½˜í…ì¸  ì„±ê²©
- ìš°ë¦¬ëŠ” "ì •ë³´ ìš”ì•½í˜• ë¯¸ë””ì–´"ì…ë‹ˆë‹¤.
- ë…ìì—ê²Œ íŒë‹¨ì„ ê°•ìš”í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
- íŒ©íŠ¸ë¥¼ ì œê³µí•˜ê³ , íŒë‹¨ì€ ë…ìì˜ ëª«ì…ë‹ˆë‹¤.

## ì–´ì¡° ê°€ì´ë“œ
âœ… ì¢‹ì€ ì˜ˆ:
- "íŠ¸ëŸ¼í”„ ëŒ€í†µë ¹ì˜ ê´€ì„¸ ì •ì±… ë°œí‘œ í›„ ê²€ìƒ‰ëŸ‰ì´ 3ë°° ì¦ê°€í–ˆìŠµë‹ˆë‹¤."
- "ì»¤ë®¤ë‹ˆí‹° ë°˜ì‘ì€ ê¸ì • 45%, ë¶€ì • 40%ë¡œ ì—‡ê°ˆë¦¬ëŠ” ëª¨ìŠµì…ë‹ˆë‹¤."
- "ì „ë¬¸ê°€ë“¤ì€ ì´ ì •ì±…ì´ êµ­ë‚´ ì‚°ì—…ì— ì˜í–¥ì„ ì¤„ ìˆ˜ ìˆë‹¤ê³  ë¶„ì„í•©ë‹ˆë‹¤."

âŒ ë‚˜ìœ ì˜ˆ:
- "ë˜ë‹¤ì‹œ ì¶©ê²©ì ì¸ ë°œí‘œê°€ ìˆì—ˆìŠµë‹ˆë‹¤!" (ìê·¹ì )
- "ë„¤í‹°ì¦Œë“¤ì´ ë¶„ë…¸í•˜ê³  ìˆìŠµë‹ˆë‹¤" (ì„ ë™ì )
- "ì´ ì •ì±…ì€ ëª…ë°±íˆ ì˜ëª»ë˜ì—ˆìŠµë‹ˆë‹¤" (ì˜ê²¬ ê°•ìš”)
- "ëª¨ë‘ê°€ ì´ ì†Œì‹ì— í™˜í˜¸í•˜ê³  ìˆìŠµë‹ˆë‹¤" (ê³¼ì¥)

## í•µì‹¬ ì§ˆë¬¸ (ì½˜í…ì¸  ì‘ì„± ì „ ìë¬¸)
1. ì´ ë¬¸ì¥ì´ íŠ¹ì • ì…ì¥ì„ ì˜¹í˜¸í•˜ê±°ë‚˜ ë¹„ë‚œí•˜ëŠ”ê°€?
2. ê²€ì¦ë˜ì§€ ì•Šì€ ì‚¬ì‹¤ì„ ë‹¨ì •ì ìœ¼ë¡œ ì„œìˆ í•˜ëŠ”ê°€?
3. ë…ìê°€ ìŠ¤ìŠ¤ë¡œ íŒë‹¨í•  ì—¬ì§€ë¥¼ ë‚¨ê¸°ëŠ”ê°€?
"""

SYSTEM_PROMPT = f"""ë‹¹ì‹ ì€ ì¤‘ë¦½ì ì´ê³  ê°ê´€ì ì¸ ë‰´ìŠ¤ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

{CONTENT_POLICY}

{CONTENT_TONE_GUIDE}

## ë‹¹ì‹ ì˜ ì—­í• :
1. ì œê³µëœ ë‰´ìŠ¤/ì—¬ë¡  ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ í•µì‹¬ ë‚´ìš©ì„ ìš”ì•½í•©ë‹ˆë‹¤.
2. ê°ì •ì ì´ê±°ë‚˜ í¸í–¥ëœ í‘œí˜„ì„ ë°°ì œí•˜ê³  ì‚¬ì‹¤ì— ê¸°ë°˜í•œ ë¶„ì„ì„ ì œê³µí•©ë‹ˆë‹¤.
3. ë‹¤ì–‘í•œ ê´€ì ì„ ê· í˜• ìˆê²Œ ë°˜ì˜í•©ë‹ˆë‹¤.
4. ë°˜ë“œì‹œ ì§€ì •ëœ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•©ë‹ˆë‹¤.
"""

USER_PROMPT_TEMPLATE = """## ë¶„ì„ ëŒ€ìƒ í‚¤ì›Œë“œ: {keyword}

## ìˆ˜ì§‘ëœ ë‰´ìŠ¤/ì—¬ë¡  ë°ì´í„°:
{context}

## ì¶œë ¥ í˜•ì‹:
ë°˜ë“œì‹œ ì•„ë˜ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”. ë‹¤ë¥¸ í…ìŠ¤íŠ¸ ì—†ì´ JSONë§Œ ì¶œë ¥í•˜ì„¸ìš”.

```json
{{
    "main_cause": "ì´ í‚¤ì›Œë“œê°€ í™”ì œê°€ ëœ í•µì‹¬ ì›ì¸ì„ 1ë¬¸ì¥ìœ¼ë¡œ ì„¤ëª…",
    "sentiment_ratio": {{
        "positive": 0.0~1.0 ì‚¬ì´ ìˆ«ì (ê¸ì • ë¹„ìœ¨),
        "negative": 0.0~1.0 ì‚¬ì´ ìˆ«ì (ë¶€ì • ë¹„ìœ¨),
        "neutral": 0.0~1.0 ì‚¬ì´ ìˆ«ì (ì¤‘ë¦½ ë¹„ìœ¨)
    }},
    "key_opinions": [
        "í•µì‹¬ ì˜ê²¬ 1",
        "í•µì‹¬ ì˜ê²¬ 2",
        "í•µì‹¬ ì˜ê²¬ 3"
    ],
    "summary": "3ì¤„ ìš”ì•½ (ì¤„ë°”ê¿ˆì€ \\n ì‚¬ìš©)"
}}
```

ì£¼ì˜ì‚¬í•­:
- ê°ì„± ë¹„ìœ¨ì˜ í•©ì€ 1.0ì´ ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.
- í•µì‹¬ ì˜ê²¬ì€ ìµœì†Œ 3ê°œ, ìµœëŒ€ 5ê°œì…ë‹ˆë‹¤.
- ìš”ì•½ì€ 80-120ì ë‚´ì™¸ë¡œ ì‘ì„±í•˜ì„¸ìš”.
- íŠ¹ì • ì¸ë¬¼ì´ë‚˜ ì •ë‹¹ì— ëŒ€í•œ ì§ì ‘ì ì¸ í‰ê°€ë¥¼ í”¼í•˜ì„¸ìš”.
"""


# =============================================================================
# Retry Configuration
# =============================================================================

class RetryConfig(BaseModel):
    """ì¬ì‹œë„ ì„¤ì •"""
    max_attempts: int = 3
    base_delay: float = 1.0
    max_delay: float = 30.0
    exponential_base: float = 2.0


# =============================================================================
# LLM Analyzer
# =============================================================================

class LLMAnalyzer:
    """
    vLLM ê¸°ë°˜ ë‰´ìŠ¤ ë¶„ì„ê¸°
    
    Blueprint Week 2 í•µì‹¬ ì»´í¬ë„ŒíŠ¸:
    - AsyncOpenAI í´ë¼ì´ì–¸íŠ¸ë¡œ vLLM ì„œë²„ ì—°ë™
    - ì¤‘ë¦½ì  ë¶„ì„ê°€ í˜ë¥´ì†Œë‚˜
    - JSON êµ¬ì¡°í™” ì¶œë ¥
    
    Week 4ì—ì„œ Outlines ì ìš©ìœ¼ë¡œ JSON 100% ë³´ì¥ ì˜ˆì •
    """
    
    def __init__(
        self,
        retry_config: RetryConfig | None = None,
        temperature: float = 0.3,
        max_tokens: int = 1024,
    ):
        self._settings = get_settings()
        self._retry_config = retry_config or RetryConfig()
        self._temperature = temperature
        self._max_tokens = max_tokens
        self._client: AsyncOpenAI | None = None
    
    @property
    def client(self) -> AsyncOpenAI:
        """AsyncOpenAI í´ë¼ì´ì–¸íŠ¸ (lazy initialization)"""
        if self._client is None:
            # vLLMì€ OpenAI API í˜¸í™˜ - base_urlë§Œ ë³€ê²½
            self._client = AsyncOpenAI(
                base_url=f"{self._settings.vllm_url}/v1",
                api_key="EMPTY",  # vLLMì€ API key ë¶ˆí•„ìš”
            )
            logger.info(
                "LLM client initialized",
                extra={
                    "base_url": self._settings.vllm_url,
                    "model": self._settings.vllm_model,
                }
            )
        return self._client
    
    def _build_context(self, articles: list[dict[str, Any]]) -> str:
        """ë‰´ìŠ¤ ê¸°ì‚¬ë“¤ì„ ì»¨í…ìŠ¤íŠ¸ ë¬¸ìì—´ë¡œ ë³€í™˜"""
        context_parts: list[str] = []
        
        for i, article in enumerate(articles, 1):
            title = article.get("title", "ì œëª© ì—†ìŒ")
            summary = article.get("summary", article.get("description", ""))
            source = article.get("source", "ì•Œ ìˆ˜ ì—†ìŒ")
            published = article.get("published", article.get("published_at", ""))
            
            part = f"[ê¸°ì‚¬ {i}]\nì œëª©: {title}"
            if summary:
                # ìš”ì•½ì´ ë„ˆë¬´ ê¸¸ë©´ ìë¥´ê¸°
                summary = summary[:500] + "..." if len(summary) > 500 else summary
                part += f"\nìš”ì•½: {summary}"
            if source:
                part += f"\nì¶œì²˜: {source}"
            if published:
                part += f"\në°œí–‰: {published}"
            
            context_parts.append(part)
        
        return "\n\n".join(context_parts)
    
    def _parse_json_response(self, content: str) -> dict[str, Any]:
        """
        LLM ì‘ë‹µì—ì„œ JSON ì¶”ì¶œ ë° íŒŒì‹±
        
        Week 4ì—ì„œ Outlines ì ìš© ì‹œ ì´ í•¨ìˆ˜ëŠ” ë¶ˆí•„ìš”í•´ì§
        """
        # JSON ì½”ë“œ ë¸”ë¡ ì¶”ì¶œ ì‹œë„
        json_match = re.search(r'```json\s*(.*?)\s*```', content, re.DOTALL)
        if json_match:
            json_str = json_match.group(1)
        else:
            # ì½”ë“œ ë¸”ë¡ ì—†ì´ JSONë§Œ ìˆëŠ” ê²½ìš°
            json_match = re.search(r'\{[\s\S]*\}', content)
            if json_match:
                json_str = json_match.group(0)
            else:
                raise ValueError("No JSON found in response")
        
        # JSON íŒŒì‹±
        try:
            return json.loads(json_str)
        except json.JSONDecodeError as e:
            logger.warning(f"JSON parse failed: {e}", extra={"content": content[:200]})
            raise ValueError(f"Invalid JSON: {e}")
    
    async def _call_llm(
        self, 
        keyword: str, 
        context: str,
    ) -> tuple[str, float]:
        """
        vLLM ì„œë²„ì— ë¶„ì„ ìš”ì²­
        
        Returns:
            (response_content, inference_time_seconds)
        """
        start_time = datetime.now()
        
        user_prompt = USER_PROMPT_TEMPLATE.format(
            keyword=keyword,
            context=context,
        )
        
        response = await self.client.chat.completions.create(
            model=self._settings.vllm_model,
            messages=[
                {"role": "system", "content": SYSTEM_PROMPT},
                {"role": "user", "content": user_prompt},
            ],
            temperature=self._temperature,
            max_tokens=self._max_tokens,
        )
        
        inference_time = (datetime.now() - start_time).total_seconds()
        content = response.choices[0].message.content or ""
        
        logger.debug(
            "LLM response received",
            extra={
                "inference_time": inference_time,
                "response_length": len(content),
                "finish_reason": response.choices[0].finish_reason,
            }
        )
        
        return content, inference_time
    
    async def _analyze_with_retry(
        self,
        keyword: str,
        context: str,
    ) -> tuple[AnalysisOutput, float]:
        """ì¬ì‹œë„ ë¡œì§ì´ ì ìš©ëœ ë¶„ì„ ìˆ˜í–‰"""
        config = self._retry_config
        last_exception: Exception | None = None
        total_inference_time = 0.0
        
        for attempt in range(config.max_attempts):
            try:
                # LLM í˜¸ì¶œ
                content, inference_time = await self._call_llm(keyword, context)
                total_inference_time += inference_time
                
                # JSON íŒŒì‹±
                json_data = self._parse_json_response(content)
                
                # Pydantic ê²€ì¦
                analysis = AnalysisOutput.model_validate(json_data)
                
                logger.info(
                    "Analysis completed successfully",
                    extra={
                        "keyword": keyword,
                        "attempt": attempt + 1,
                        "inference_time": total_inference_time,
                    }
                )
                
                return analysis, total_inference_time
                
            except (ValueError, json.JSONDecodeError) as e:
                # JSON íŒŒì‹± ì‹¤íŒ¨ - ì¬ì‹œë„
                last_exception = e
                logger.warning(
                    f"JSON parsing failed, attempt {attempt + 1}/{config.max_attempts}",
                    extra={"error": str(e), "keyword": keyword}
                )
                
            except (APIError, APIConnectionError) as e:
                # API ì˜¤ë¥˜ - ì¬ì‹œë„
                last_exception = e
                logger.warning(
                    f"API error, attempt {attempt + 1}/{config.max_attempts}",
                    extra={"error": str(e), "keyword": keyword}
                )
                
            except RateLimitError as e:
                # Rate limit - ë” ê¸´ ëŒ€ê¸°
                last_exception = e
                logger.warning(
                    f"Rate limited, attempt {attempt + 1}/{config.max_attempts}",
                    extra={"error": str(e), "keyword": keyword}
                )
            
            # ì¬ì‹œë„ ëŒ€ê¸°
            if attempt < config.max_attempts - 1:
                delay = min(
                    config.base_delay * (config.exponential_base ** attempt),
                    config.max_delay
                )
                await asyncio.sleep(delay)
        
        # ëª¨ë“  ì¬ì‹œë„ ì‹¤íŒ¨
        raise RuntimeError(
            f"Analysis failed after {config.max_attempts} attempts: {last_exception}"
        )
    
    async def analyze(
        self,
        keyword: str,
        articles: list[dict[str, Any]],
    ) -> AnalysisResult:
        """
        ë‰´ìŠ¤ ê¸°ì‚¬ë“¤ì„ ë¶„ì„í•˜ì—¬ ìš”ì•½ ê²°ê³¼ ë°˜í™˜
        
        Args:
            keyword: ë¶„ì„ ëŒ€ìƒ í‚¤ì›Œë“œ
            articles: ë‰´ìŠ¤ ê¸°ì‚¬ ëª©ë¡ (dict í˜•íƒœ)
                - title: ê¸°ì‚¬ ì œëª©
                - summary/description: ê¸°ì‚¬ ìš”ì•½
                - source: ì¶œì²˜
                - published/published_at: ë°œí–‰ì¼
        
        Returns:
            AnalysisResult: ë¶„ì„ ê²°ê³¼
            
        Raises:
            RuntimeError: ë¶„ì„ ì‹¤íŒ¨ ì‹œ
            ValueError: ì…ë ¥ ë°ì´í„° ê²€ì¦ ì‹¤íŒ¨ ì‹œ
        """
        if not articles:
            raise ValueError("No articles provided for analysis")
        
        if not keyword or not keyword.strip():
            raise ValueError("Keyword cannot be empty")
        
        logger.info(
            "Starting analysis",
            extra={"keyword": keyword, "article_count": len(articles)}
        )
        
        # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        context = self._build_context(articles)
        
        # LLM ë¶„ì„ ìˆ˜í–‰
        analysis, inference_time = await self._analyze_with_retry(keyword, context)
        
        # ê²°ê³¼ êµ¬ì„±
        result = AnalysisResult(
            keyword=keyword,
            analysis=analysis,
            source_count=len(articles),
            model_version=self._settings.vllm_model,
            inference_time_seconds=inference_time,
        )
        
        logger.info(
            "Analysis result created",
            extra={
                "keyword": keyword,
                "is_valid": result.is_valid(),
                "inference_time": inference_time,
            }
        )
        
        return result
    
    async def analyze_from_collection_result(
        self,
        collection_result: Any,  # CollectionResult íƒ€ì…
    ) -> AnalysisResult:
        """
        CollectionResult ê°ì²´ë¡œë¶€í„° ì§ì ‘ ë¶„ì„ ìˆ˜í–‰
        
        collector_rss_google.pyì˜ CollectionResultì™€ ì—°ë™
        """
        # CollectionResultì˜ articlesë¥¼ dictë¡œ ë³€í™˜
        articles = [
            {
                "title": article.title,
                "summary": article.summary,
                "source": article.source,
                "published": article.published.isoformat() if article.published else None,
            }
            for article in collection_result.articles
        ]
        
        return await self.analyze(
            keyword=collection_result.keyword,
            articles=articles,
        )
    
    async def close(self) -> None:
        """í´ë¼ì´ì–¸íŠ¸ ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        if self._client is not None:
            await self._client.close()
            self._client = None
            logger.info("LLM client closed")
    
    async def __aenter__(self) -> "LLMAnalyzer":
        return self
    
    async def __aexit__(self, exc_type: Any, exc_val: Any, exc_tb: Any) -> None:
        await self.close()


# =============================================================================
# Convenience Functions
# =============================================================================

async def analyze_keyword(
    keyword: str,
    articles: list[dict[str, Any]],
) -> AnalysisResult:
    """
    ë‹¨ì¼ í‚¤ì›Œë“œ ë¶„ì„ í¸ì˜ í•¨ìˆ˜
    
    Usage:
        result = await analyze_keyword(
            keyword="íŠ¸ëŸ¼í”„ ê´€ì„¸",
            articles=[
                {"title": "...", "summary": "...", "source": "..."},
                ...
            ]
        )
    """
    async with LLMAnalyzer() as analyzer:
        return await analyzer.analyze(keyword, articles)


# =============================================================================
# CLI Test
# =============================================================================

if __name__ == "__main__":
    async def main() -> None:
        """í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        # í…ŒìŠ¤íŠ¸ìš© ë”ë¯¸ ë‰´ìŠ¤ ë°ì´í„°
        test_articles = [
            {
                "title": "íŠ¸ëŸ¼í”„, ì¤‘êµ­ì‚° ì œí’ˆ 25% ê´€ì„¸ ë¶€ê³¼ ë°œí‘œ",
                "summary": "ë¯¸êµ­ ëŒ€í†µë ¹ì´ ë¬´ì—­ ì „ìŸ ê²©í™” ì†ì—ì„œ ìƒˆë¡œìš´ ê´€ì„¸ ì •ì±…ì„ ë°œí‘œí–ˆë‹¤. ì´ì— ë”°ë¼ ì¤‘êµ­ì‚° ì œí’ˆì— 25%ì˜ ì¶”ê°€ ê´€ì„¸ê°€ ë¶€ê³¼ë  ì˜ˆì •ì´ë‹¤.",
                "source": "ê²½ì œì¼ë³´",
                "published": "2025-02-15T09:00:00",
            },
            {
                "title": "êµ­ë‚´ ìˆ˜ì¶œê¸°ì—…ë“¤ 'ë¹„ìƒ'â€¦ë°˜ë„ì²´Â·ë°°í„°ë¦¬ ì—…ì¢… íƒ€ê²© ìš°ë ¤",
                "summary": "ë¯¸êµ­ì˜ ê´€ì„¸ ì •ì±… ë°œí‘œ ì´í›„ êµ­ë‚´ ìˆ˜ì¶œ ê¸°ì—…ë“¤ì´ ë¹„ìƒ ëŒ€ì‘ì— ë‚˜ì„°ë‹¤. íŠ¹íˆ ë°˜ë„ì²´ì™€ ë°°í„°ë¦¬ ì—…ì¢…ì´ ì§ì ‘ì ì¸ íƒ€ê²©ì„ ë°›ì„ ê²ƒìœ¼ë¡œ ì „ë§ëœë‹¤.",
                "source": "ì‚°ì—…ë‰´ìŠ¤",
                "published": "2025-02-15T10:30:00",
            },
            {
                "title": "ì „ë¬¸ê°€ \"ë¬´ì—­ì „ìŸ ì¥ê¸°í™” ì‹œ êµ­ë‚´ GDP 0.5%p í•˜ë½ ê°€ëŠ¥\"",
                "summary": "ê²½ì œ ì „ë¬¸ê°€ë“¤ì€ ë¯¸ì¤‘ ë¬´ì—­ì „ìŸì´ ì¥ê¸°í™”ë  ê²½ìš° êµ­ë‚´ ê²½ì œì— ìƒë‹¹í•œ ì˜í–¥ì„ ë¯¸ì¹  ê²ƒìœ¼ë¡œ ë¶„ì„í–ˆë‹¤.",
                "source": "ê²½ì œì—°êµ¬ì†Œ",
                "published": "2025-02-15T11:00:00",
            },
            {
                "title": "ì¦ì‹œ ê¸‰ë½â€¦ì½”ìŠ¤í”¼ 2% ì´ìƒ í•˜ë½ ë§ˆê°",
                "summary": "ê´€ì„¸ ì •ì±… ë°œí‘œ ì—¬íŒŒë¡œ êµ­ë‚´ ì¦ì‹œê°€ ê¸‰ë½í–ˆë‹¤. ì½”ìŠ¤í”¼ëŠ” 2% ì´ìƒ í•˜ë½í•˜ë©° íˆ¬ììë“¤ì˜ ë¶ˆì•ˆê°ì´ ì»¤ì§€ê³  ìˆë‹¤.",
                "source": "ì¦ê¶Œíƒ€ì„ìŠ¤",
                "published": "2025-02-15T15:30:00",
            },
            {
                "title": "ì •ë¶€ \"ìˆ˜ì¶œê¸°ì—… ì§€ì› ëŒ€ì±… ë§ˆë ¨ ì¤‘\"",
                "summary": "ì •ë¶€ëŠ” ë¯¸êµ­ì˜ ê´€ì„¸ ì •ì±…ì— ëŒ€ì‘í•˜ì—¬ ìˆ˜ì¶œ ê¸°ì—… ì§€ì› ëŒ€ì±…ì„ ë§ˆë ¨ ì¤‘ì´ë¼ê³  ë°í˜”ë‹¤.",
                "source": "ì •ì±…ë¸Œë¦¬í•‘",
                "published": "2025-02-15T16:00:00",
            },
        ]
        
        print("\n" + "=" * 60)
        print("  LLM Analyzer Test")
        print("=" * 60)
        
        try:
            result = await analyze_keyword(
                keyword="íŠ¸ëŸ¼í”„ ê´€ì„¸",
                articles=test_articles,
            )
            
            print(f"\nâœ… ë¶„ì„ ì™„ë£Œ!")
            print(f"\nğŸ“Š ë¶„ì„ ê²°ê³¼:")
            print(f"   í‚¤ì›Œë“œ: {result.keyword}")
            print(f"   ì†ŒìŠ¤ ìˆ˜: {result.source_count}")
            print(f"   ëª¨ë¸: {result.model_version}")
            print(f"   ì¶”ë¡  ì‹œê°„: {result.inference_time_seconds:.2f}ì´ˆ")
            print(f"\nğŸ“ í•µì‹¬ ì›ì¸:")
            print(f"   {result.analysis.main_cause}")
            print(f"\nğŸ“ˆ ê°ì„± ë¹„ìœ¨:")
            print(f"   ê¸ì •: {result.analysis.sentiment_ratio.positive:.0%}")
            print(f"   ë¶€ì •: {result.analysis.sentiment_ratio.negative:.0%}")
            print(f"   ì¤‘ë¦½: {result.analysis.sentiment_ratio.neutral:.0%}")
            print(f"\nğŸ’¬ í•µì‹¬ ì˜ê²¬:")
            for i, opinion in enumerate(result.analysis.key_opinions, 1):
                print(f"   {i}. {opinion}")
            print(f"\nğŸ“„ 3ì¤„ ìš”ì•½:")
            for line in result.analysis.summary.split("\n"):
                print(f"   {line}")
            print(f"\n   ìœ íš¨ì„± ê²€ì‚¬: {'âœ… í†µê³¼' if result.is_valid() else 'âŒ ì‹¤íŒ¨'}")
            
        except Exception as e:
            print(f"\nâŒ ë¶„ì„ ì‹¤íŒ¨: {e}")
            raise
    
    asyncio.run(main())