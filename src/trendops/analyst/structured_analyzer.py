# src/trendops/analyst/structured_analyzer.py
"""
Week 4 Day 1: Outlines + Ollama í†µí•© Structured Analyzer

Blueprint Week 4 Goal:
- JSON ì¶œë ¥ 100% ë³´ì¥ (Outlines guided decoding)
- 7B ëª¨ë¸ì˜ ì¶œë ¥ ë¶ˆì•ˆì • í•´ê²°
- Retry ë¶ˆí•„ìš”í•œ í™•ì •ì  JSON ìƒì„±

í•µì‹¬ ì„¤ê³„:
1. Primary: Outlines + Ollama (ë¬¸ë²•ì  JSON ê°•ì œ)
2. Fallback: Ollama JSON ëª¨ë“œ + Pydantic validation
3. ê¸°ì¡´ analyzer_llm.py ìŠ¤í‚¤ë§ˆ ì™„ì „ í˜¸í™˜
"""

from __future__ import annotations

import asyncio
import json
import re
import time
from abc import ABC, abstractmethod
from datetime import datetime
from typing import Any, TypeVar

from pydantic import BaseModel, Field, ValidationError, field_validator

# =============================================================================
# Pydantic Schemas (analyzer_llm.pyì™€ í˜¸í™˜)
# =============================================================================


class SentimentRatio(BaseModel):
    """ê°ì„± ë¹„ìœ¨ ìŠ¤í‚¤ë§ˆ"""

    positive: float = Field(..., ge=0.0, le=1.0, description="ê¸ì • ë¹„ìœ¨")
    negative: float = Field(..., ge=0.0, le=1.0, description="ë¶€ì • ë¹„ìœ¨")
    neutral: float = Field(..., ge=0.0, le=1.0, description="ì¤‘ë¦½ ë¹„ìœ¨")

    @field_validator("positive", "negative", "neutral", mode="after")
    @classmethod
    def round_ratio(cls, v: float) -> float:
        return round(v, 2)

    def model_post_init(self, __context: Any) -> None:
        """ë¹„ìœ¨ í•©ì´ 1.0ì´ ë˜ë„ë¡ ì •ê·œí™”"""
        total = self.positive + self.negative + self.neutral
        if total > 0 and abs(total - 1.0) > 0.01:
            self.positive = round(self.positive / total, 2)
            self.negative = round(self.negative / total, 2)
            self.neutral = round(1.0 - self.positive - self.negative, 2)


class AnalysisOutput(BaseModel):
    """
    LLM ë¶„ì„ ì¶œë ¥ ìŠ¤í‚¤ë§ˆ (Outlinesìš©)

    Week 4: Outlinesê°€ ì´ ìŠ¤í‚¤ë§ˆë¥¼ ê¸°ë°˜ìœ¼ë¡œ JSON ë¬¸ë²• ê°•ì œ
    """

    main_cause: str = Field(
        ..., min_length=10, max_length=200, description="ì´ í‚¤ì›Œë“œê°€ ëœ¬ í•µì‹¬ ì›ì¸ (1ë¬¸ì¥)"
    )
    sentiment_ratio: SentimentRatio = Field(..., description="ì—¬ë¡  ê°ì„± ë¹„ìœ¨")
    key_opinions: list[str] = Field(..., min_length=3, max_length=5, description="í•µì‹¬ ì˜ê²¬ 3-5ê°œ")
    summary: str = Field(..., min_length=50, max_length=300, description="3ì¤„ ìš”ì•½")


class AnalysisResult(BaseModel):
    """ë¶„ì„ ê²°ê³¼ ì „ì²´ ìŠ¤í‚¤ë§ˆ"""

    keyword: str = Field(..., description="ë¶„ì„ ëŒ€ìƒ í‚¤ì›Œë“œ")
    analysis: AnalysisOutput = Field(..., description="LLM ë¶„ì„ ê²°ê³¼")
    source_count: int = Field(..., ge=0, description="ë¶„ì„ì— ì‚¬ìš©ëœ ì†ŒìŠ¤ ìˆ˜")
    model_version: str = Field(..., description="ì‚¬ìš©ëœ ëª¨ë¸ ë²„ì „")
    inference_time_seconds: float = Field(..., ge=0, description="ì¶”ë¡  ì†Œìš” ì‹œê°„")
    generation_method: str = Field(default="outlines", description="ìƒì„± ë°©ì‹")
    created_at: datetime = Field(default_factory=datetime.now, description="ìƒì„± ì‹œê°„")

    def is_valid(self) -> bool:
        """ë¶„ì„ ê²°ê³¼ ìœ íš¨ì„± ê²€ì‚¬"""
        return (
            len(self.analysis.main_cause) >= 10
            and len(self.analysis.key_opinions) >= 3
            and len(self.analysis.summary) >= 50
        )


# =============================================================================
# Prompts (Blueprint Section 1.2.3 & 1.3.3)
# =============================================================================

SYSTEM_PROMPT = """ë‹¹ì‹ ì€ ì¤‘ë¦½ì ì´ê³  ê°ê´€ì ì¸ ë‰´ìŠ¤ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

## í•„ìˆ˜ ì¤€ìˆ˜ ì‚¬í•­:
1. íŠ¹ì • ì •ì¹˜ì¸/ì •ë‹¹ì„ "ì¢‹ë‹¤/ë‚˜ì˜ë‹¤"ë¡œ í‰ê°€í•˜ì§€ ë§ˆì„¸ìš”.
2. ì›ë¬¸ ê¸°ì‚¬ë¥¼ ê·¸ëŒ€ë¡œ ì¸ìš©í•˜ì§€ ë§ê³ , í†µê³„ì  ìš”ì•½ë§Œ ì œê³µí•˜ì„¸ìš”.
3. "~í•œ ê²ƒìœ¼ë¡œ ì•Œë ¤ì¡Œë‹¤", "~ë¼ëŠ” ì˜ê²¬ì´ ìˆë‹¤" í˜•íƒœì˜ ê°ê´€ì  ì„œìˆ ë§Œ ì‚¬ìš©í•˜ì„¸ìš”.
4. ê²€ì¦ë˜ì§€ ì•Šì€ ì‚¬ì‹¤ì„ ë‹¨ì •ì ìœ¼ë¡œ ì„œìˆ í•˜ì§€ ë§ˆì„¸ìš”.
5. ê°œì¸ì„ íŠ¹ì •í•  ìˆ˜ ìˆëŠ” ì •ë³´(ì´ë¦„, ì—°ë½ì²˜ ë“±)ëŠ” ì ˆëŒ€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.

## ì½˜í…ì¸  ì„±ê²©:
- ì´ ì½˜í…ì¸ ëŠ” "ì •ë³´ ìš”ì•½í˜•"ì…ë‹ˆë‹¤.
- ìš°ë¦¬ì˜ ì—­í• ì€ "í˜„ìƒ ì„¤ëª…"ì´ì§€, "ì˜ê²¬ ì œì‹œ"ê°€ ì•„ë‹™ë‹ˆë‹¤.
- ë…ìê°€ ìŠ¤ìŠ¤ë¡œ íŒë‹¨í•  ìˆ˜ ìˆë„ë¡ ê· í˜• ì¡íŒ ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤.

## ë‹¹ì‹ ì˜ ì—­í• :
1. ì œê³µëœ ë‰´ìŠ¤/ì—¬ë¡  ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ í•µì‹¬ ë‚´ìš©ì„ ìš”ì•½í•©ë‹ˆë‹¤.
2. ê°ì •ì ì´ê±°ë‚˜ í¸í–¥ëœ í‘œí˜„ì„ ë°°ì œí•˜ê³  ì‚¬ì‹¤ì— ê¸°ë°˜í•œ ë¶„ì„ì„ ì œê³µí•©ë‹ˆë‹¤.
3. ë‹¤ì–‘í•œ ê´€ì ì„ ê· í˜• ìˆê²Œ ë°˜ì˜í•©ë‹ˆë‹¤.
"""


def build_user_prompt(keyword: str, context: str) -> str:
    """ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ ìƒì„±"""
    return f"""## ë¶„ì„ ëŒ€ìƒ í‚¤ì›Œë“œ: {keyword}

## ìˆ˜ì§‘ëœ ë‰´ìŠ¤/ì—¬ë¡  ë°ì´í„°:
{context}

## ë¶„ì„ ì§€ì¹¨:
1. main_cause: ì´ í‚¤ì›Œë“œê°€ í™”ì œê°€ ëœ í•µì‹¬ ì›ì¸ì„ 1ë¬¸ì¥ìœ¼ë¡œ ì„¤ëª…
2. sentiment_ratio: ê¸ì •/ë¶€ì •/ì¤‘ë¦½ ë¹„ìœ¨ (í•©ì´ 1.0)
3. key_opinions: ì£¼ìš” ì—¬ë¡ /ì˜ê²¬ 3-5ê°œ
4. summary: ì „ì²´ ìƒí™©ì„ 3ì¤„ë¡œ ìš”ì•½ (ì¤„ë°”ê¿ˆì€ \\n ì‚¬ìš©)

JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”."""


# =============================================================================
# Generation Backend Interface
# =============================================================================

T = TypeVar("T", bound=BaseModel)


class GenerationBackend(ABC):
    """ìƒì„± ë°±ì—”ë“œ ì¶”ìƒ í´ë˜ìŠ¤"""

    @abstractmethod
    async def generate(
        self,
        prompt: str,
        schema: type[T],
        system_prompt: str | None = None,
    ) -> T:
        """ìŠ¤í‚¤ë§ˆì— ë§ëŠ” êµ¬ì¡°í™”ëœ ì¶œë ¥ ìƒì„±"""
        pass

    @abstractmethod
    def get_name(self) -> str:
        """ë°±ì—”ë“œ ì´ë¦„ ë°˜í™˜"""
        pass


class OutlinesOllamaBackend(GenerationBackend):
    """
    Outlines + Ollama ë°±ì—”ë“œ

    JSON ë¬¸ë²•ì„ ê°•ì œí•˜ì—¬ 100% ìœ íš¨í•œ JSON ì¶œë ¥ ë³´ì¥
    """

    def __init__(
        self,
        model_name: str = "exaone3.5",
        base_url: str = "http://localhost:11434",
    ):
        self.model_name = model_name
        self.base_url = base_url
        self._model = None
        self._generator_cache: dict[type, Any] = {}

    def _get_model(self):
        """Outlines ëª¨ë¸ lazy loading (í˜¸í™˜ì„± ê°œì„  íŒ¨ì¹˜)"""
        if self._model is None:
            try:
                from outlines import models

                # 1. models.ollamaê°€ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸ (ìµœì‹  ë²„ì „ outlines)
                if hasattr(models, "ollama"):
                    self._model = models.ollama(
                        self.model_name,
                        base_url=self.base_url,
                    )
                # 2. ì—†ë‹¤ë©´ OpenAI í˜¸í™˜ ëª¨ë“œë¡œ ì—°ê²° (êµ¬ë²„ì „ outlines ëŒ€ì‘)
                # OllamaëŠ” http://localhost:11434/v1 ì—ì„œ OpenAI APIì™€ í˜¸í™˜ë©ë‹ˆë‹¤.
                else:
                    # URL ëì— /v1ì´ ì—†ìœ¼ë©´ ì¶”ê°€
                    base_url = self.base_url.rstrip("/")
                    if not base_url.endswith("/v1"):
                        base_url += "/v1"

                    self._model = models.openai(
                        self.model_name,
                        base_url=base_url,
                        api_key="ollama",  # ë”ë¯¸ í‚¤ (OllamaëŠ” í‚¤ ê²€ì‚¬ ì•ˆí•¨)
                    )

            except ImportError:
                raise ImportError("outlines ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤: pip install outlines")
            except Exception as e:
                # ìƒì„¸ ì—ëŸ¬ ë¡œê¹…
                print(f"[DEBUG] Outlines Init Error: {e}")
                raise RuntimeError(f"Ollama ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")

        return self._model

    def _get_generator(self, schema: type[T]):
        """ìŠ¤í‚¤ë§ˆë³„ generator ìºì‹±"""
        if schema not in self._generator_cache:
            from outlines import generate

            model = self._get_model()
            self._generator_cache[schema] = generate.json(model, schema)
        return self._generator_cache[schema]

    async def generate(
        self,
        prompt: str,
        schema: type[T],
        system_prompt: str | None = None,
    ) -> T:
        """Outlinesë¥¼ ì‚¬ìš©í•œ êµ¬ì¡°í™”ëœ JSON ìƒì„±"""
        generator = self._get_generator(schema)

        # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ì™€ ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ ê²°í•©
        full_prompt = prompt
        if system_prompt:
            full_prompt = f"{system_prompt}\n\n{prompt}"

        # OutlinesëŠ” ë™ê¸° í•¨ìˆ˜ì´ë¯€ë¡œ executorì—ì„œ ì‹¤í–‰
        loop = asyncio.get_event_loop()
        result = await loop.run_in_executor(
            None,
            generator,
            full_prompt,
        )

        return result

    def get_name(self) -> str:
        return f"outlines-ollama:{self.model_name}"


class OllamaJsonModeBackend(GenerationBackend):
    """
    Ollama JSON ëª¨ë“œ ë°±ì—”ë“œ (Fallback)

    Ollamaì˜ native JSON ëª¨ë“œ + Pydantic validation
    Outlinesê°€ ì‹¤íŒ¨í•  ê²½ìš° ì‚¬ìš©
    """

    def __init__(
        self,
        model_name: str = "exaone3.5",
        base_url: str = "http://localhost:11434",
        max_retries: int = 3,
    ):
        self.model_name = model_name
        self.base_url = base_url
        self.max_retries = max_retries
        self._client = None

    def _get_client(self):
        """Ollama í´ë¼ì´ì–¸íŠ¸ lazy loading"""
        if self._client is None:
            try:
                from ollama import AsyncClient

                self._client = AsyncClient(host=self.base_url)
            except ImportError:
                raise ImportError("ollama ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤: pip install ollama")
        return self._client

    async def generate(
        self,
        prompt: str,
        schema: type[T],
        system_prompt: str | None = None,
    ) -> T:
        """Ollama JSON ëª¨ë“œë¥¼ ì‚¬ìš©í•œ ìƒì„± + Pydantic ê²€ì¦"""
        client = self._get_client()

        messages = []
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        messages.append({"role": "user", "content": prompt})

        last_error: Exception | None = None

        for attempt in range(self.max_retries):
            try:
                response = await client.chat(
                    model=self.model_name,
                    messages=messages,
                    format="json",  # Ollama JSON ëª¨ë“œ
                    options={
                        "temperature": 0.3,
                        "num_predict": 2048,
                    },
                )

                content = response["message"]["content"]

                # JSON íŒŒì‹± ì‹œë„
                try:
                    data = json.loads(content)
                except json.JSONDecodeError:
                    # JSON ë¸”ë¡ ì¶”ì¶œ ì‹œë„
                    json_match = re.search(r"```json\s*(.*?)\s*```", content, re.DOTALL)
                    if json_match:
                        data = json.loads(json_match.group(1))
                    else:
                        # ì¤‘ê´„í˜¸ë¡œ ì‹œì‘í•˜ëŠ” ë¶€ë¶„ ì¶”ì¶œ
                        json_start = content.find("{")
                        json_end = content.rfind("}") + 1
                        if json_start >= 0 and json_end > json_start:
                            data = json.loads(content[json_start:json_end])
                        else:
                            raise ValueError("JSONì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

                # Pydantic ê²€ì¦
                return schema.model_validate(data)

            except (json.JSONDecodeError, ValidationError) as e:
                last_error = e
                # ì—ëŸ¬ í”¼ë“œë°±ê³¼ í•¨ê»˜ ì¬ì‹œë„
                if attempt < self.max_retries - 1:
                    feedback_msg = f"ì´ì „ ì‘ë‹µì´ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ì˜¤ë¥˜: {str(e)}\n\nì˜¬ë°”ë¥¸ JSON í˜•ì‹ìœ¼ë¡œ ë‹¤ì‹œ ì‘ë‹µí•´ì£¼ì„¸ìš”."
                    messages.append({"role": "assistant", "content": content})
                    messages.append({"role": "user", "content": feedback_msg})
                continue
            except Exception as e:
                last_error = e
                break

        raise RuntimeError(f"JSON ìƒì„± ì‹¤íŒ¨ (ì‹œë„ {self.max_retries}íšŒ): {last_error}")

    def get_name(self) -> str:
        return f"ollama-json:{self.model_name}"


# =============================================================================
# Structured Analyzer (Main Class)
# =============================================================================


class StructuredAnalyzer:
    """
    Week 4 í•µì‹¬: êµ¬ì¡°í™”ëœ ì¶œë ¥ ë³´ì¥ ë¶„ì„ê¸°

    íŠ¹ì§•:
    1. Outlinesë¥¼ ì‚¬ìš©í•œ JSON ë¬¸ë²• ê°•ì œ (Primary)
    2. Ollama JSON ëª¨ë“œ fallback (Secondary)
    3. ê¸°ì¡´ analyzer_llm.pyì™€ ì™„ì „ í˜¸í™˜

    Usage:
        async with StructuredAnalyzer() as analyzer:
            result = await analyzer.analyze(keyword, articles)
    """

    def __init__(
        self,
        model_name: str = "exaone3.5",
        base_url: str = "http://localhost:11434",
        use_outlines: bool = True,
    ):
        """
        Args:
            model_name: Ollama ëª¨ë¸ ì´ë¦„
            base_url: Ollama ì„œë²„ URL
            use_outlines: Outlines ì‚¬ìš© ì—¬ë¶€ (Falseë©´ JSON ëª¨ë“œë§Œ ì‚¬ìš©)
        """
        self.model_name = model_name
        self.base_url = base_url
        self.use_outlines = use_outlines

        # ë°±ì—”ë“œ ì´ˆê¸°í™”
        self._primary_backend: GenerationBackend | None = None
        self._fallback_backend: GenerationBackend | None = None
        self._backend_initialized = False

    def _init_backends(self) -> None:
        """ë°±ì—”ë“œ lazy ì´ˆê¸°í™”"""
        if self._backend_initialized:
            return

        # Primary: Outlines + Ollama
        if self.use_outlines:
            try:
                self._primary_backend = OutlinesOllamaBackend(
                    model_name=self.model_name,
                    base_url=self.base_url,
                )
            except ImportError:
                print("[WARNING] Outlines ì‚¬ìš© ë¶ˆê°€, JSON ëª¨ë“œë¡œ fallback")
                self._primary_backend = None

        # Fallback: Ollama JSON ëª¨ë“œ
        self._fallback_backend = OllamaJsonModeBackend(
            model_name=self.model_name,
            base_url=self.base_url,
        )

        self._backend_initialized = True

    def _build_context(self, articles: list[dict[str, Any]]) -> str:
        """ë‰´ìŠ¤ ê¸°ì‚¬ ëª©ë¡ì„ ì»¨í…ìŠ¤íŠ¸ ë¬¸ìì—´ë¡œ ë³€í™˜"""
        context_parts = []

        for i, article in enumerate(articles[:15], 1):  # ìµœëŒ€ 15ê°œ
            title = article.get("title", "ì œëª© ì—†ìŒ")
            summary = article.get("summary") or article.get("description", "")
            source = article.get("source", "ì•Œ ìˆ˜ ì—†ìŒ")
            published = article.get("published") or article.get("published_at", "")

            # ìš”ì•½ì´ ë„ˆë¬´ ê¸¸ë©´ ìë¥´ê¸°
            if len(summary) > 300:
                summary = summary[:300] + "..."

            context_parts.append(
                f"[{i}] {title}\n"
                f"    ì¶œì²˜: {source} | ë°œí–‰: {published}\n"
                f"    ìš”ì•½: {summary}"
            )

        return "\n\n".join(context_parts)

    async def analyze(
        self,
        keyword: str,
        articles: list[dict[str, Any]],
    ) -> AnalysisResult:
        """
        ë‰´ìŠ¤ ê¸°ì‚¬ë“¤ì„ ë¶„ì„í•˜ì—¬ êµ¬ì¡°í™”ëœ ê²°ê³¼ ë°˜í™˜

        Args:
            keyword: ë¶„ì„ ëŒ€ìƒ í‚¤ì›Œë“œ
            articles: ë‰´ìŠ¤ ê¸°ì‚¬ ëª©ë¡

        Returns:
            AnalysisResult: 100% ìœ íš¨í•œ êµ¬ì¡°í™”ëœ ë¶„ì„ ê²°ê³¼
        """
        if not articles:
            raise ValueError("ë¶„ì„í•  ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤")

        if not keyword or not keyword.strip():
            raise ValueError("í‚¤ì›Œë“œê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤")

        self._init_backends()

        # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        context = self._build_context(articles)
        prompt = build_user_prompt(keyword, context)

        start_time = time.time()
        analysis: AnalysisOutput | None = None
        backend_used = ""

        # Primary ë°±ì—”ë“œ ì‹œë„
        if self._primary_backend is not None:
            try:
                analysis = await self._primary_backend.generate(
                    prompt=prompt,
                    schema=AnalysisOutput,
                    system_prompt=SYSTEM_PROMPT,
                )
                backend_used = self._primary_backend.get_name()
            except Exception as e:
                print(f"[WARNING] Primary ë°±ì—”ë“œ ì‹¤íŒ¨: {e}")

        # Fallback ë°±ì—”ë“œ ì‹œë„
        if analysis is None and self._fallback_backend is not None:
            try:
                analysis = await self._fallback_backend.generate(
                    prompt=prompt,
                    schema=AnalysisOutput,
                    system_prompt=SYSTEM_PROMPT,
                )
                backend_used = self._fallback_backend.get_name()
            except Exception as e:
                raise RuntimeError(f"ëª¨ë“  ë°±ì—”ë“œ ì‹¤íŒ¨: {e}")

        if analysis is None:
            raise RuntimeError("ë¶„ì„ ìƒì„± ì‹¤íŒ¨: ì‚¬ìš© ê°€ëŠ¥í•œ ë°±ì—”ë“œê°€ ì—†ìŠµë‹ˆë‹¤")

        inference_time = time.time() - start_time

        return AnalysisResult(
            keyword=keyword,
            analysis=analysis,
            source_count=len(articles),
            model_version=self.model_name,
            inference_time_seconds=round(inference_time, 2),
            generation_method=backend_used,
        )

    async def analyze_from_collection_result(
        self,
        collection_result: Any,
    ) -> AnalysisResult:
        """
        CollectionResult ê°ì²´ë¡œë¶€í„° ì§ì ‘ ë¶„ì„ ìˆ˜í–‰

        collector_rss_google.pyì˜ CollectionResultì™€ ì—°ë™
        """
        articles = [
            {
                "title": article.title,
                "summary": article.summary,
                "source": article.source,
                "published": article.published.isoformat() if article.published else None,
            }
            for article in collection_result.articles
        ]

        return await self.analyze(
            keyword=collection_result.keyword,
            articles=articles,
        )

    async def close(self) -> None:
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        pass  # í˜„ì¬ íŠ¹ë³„í•œ ì •ë¦¬ ë¶ˆí•„ìš”

    async def __aenter__(self) -> StructuredAnalyzer:
        return self

    async def __aexit__(self, exc_type: Any, exc_val: Any, exc_tb: Any) -> None:
        await self.close()


# =============================================================================
# Convenience Functions
# =============================================================================


async def analyze_keyword_structured(
    keyword: str,
    articles: list[dict[str, Any]],
    model_name: str = "exaone3.5",
) -> AnalysisResult:
    """
    ë‹¨ì¼ í‚¤ì›Œë“œ ë¶„ì„ í¸ì˜ í•¨ìˆ˜

    Usage:
        result = await analyze_keyword_structured(
            keyword="íŠ¸ëŸ¼í”„ ê´€ì„¸",
            articles=[
                {"title": "...", "summary": "...", "source": "..."},
                ...
            ]
        )
    """
    async with StructuredAnalyzer(model_name=model_name) as analyzer:
        return await analyzer.analyze(keyword, articles)


# =============================================================================
# CLI Test
# =============================================================================

if __name__ == "__main__":

    async def main() -> None:
        """í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        # í…ŒìŠ¤íŠ¸ìš© ë”ë¯¸ ë‰´ìŠ¤ ë°ì´í„°
        test_articles = [
            {
                "title": "íŠ¸ëŸ¼í”„, ì¤‘êµ­ì‚° ì œí’ˆ 25% ê´€ì„¸ ë¶€ê³¼ ë°œí‘œ",
                "summary": "ë¯¸êµ­ ëŒ€í†µë ¹ì´ ë¬´ì—­ ì „ìŸ ê²©í™” ì†ì—ì„œ ìƒˆë¡œìš´ ê´€ì„¸ ì •ì±…ì„ ë°œí‘œí–ˆë‹¤. ì´ì— ë”°ë¼ ì¤‘êµ­ì‚° ì œí’ˆì— 25%ì˜ ì¶”ê°€ ê´€ì„¸ê°€ ë¶€ê³¼ë  ì˜ˆì •ì´ë‹¤.",
                "source": "ê²½ì œì¼ë³´",
                "published": "2025-02-15T09:00:00",
            },
            {
                "title": "êµ­ë‚´ ìˆ˜ì¶œê¸°ì—…ë“¤ 'ë¹„ìƒ'â€¦ë°˜ë„ì²´Â·ë°°í„°ë¦¬ ì—…ì¢… íƒ€ê²© ìš°ë ¤",
                "summary": "ë¯¸êµ­ì˜ ê´€ì„¸ ì •ì±… ë°œí‘œ ì´í›„ êµ­ë‚´ ìˆ˜ì¶œ ê¸°ì—…ë“¤ì´ ë¹„ìƒ ëŒ€ì‘ì— ë‚˜ì„°ë‹¤. íŠ¹íˆ ë°˜ë„ì²´ì™€ ë°°í„°ë¦¬ ì—…ì¢…ì´ ì§ì ‘ì ì¸ íƒ€ê²©ì„ ë°›ì„ ê²ƒìœ¼ë¡œ ì „ë§ëœë‹¤.",
                "source": "ì‚°ì—…ë‰´ìŠ¤",
                "published": "2025-02-15T10:30:00",
            },
            {
                "title": 'ì „ë¬¸ê°€ "ë¬´ì—­ì „ìŸ ì¥ê¸°í™” ì‹œ êµ­ë‚´ GDP 0.5%p í•˜ë½ ê°€ëŠ¥"',
                "summary": "ê²½ì œ ì „ë¬¸ê°€ë“¤ì€ ë¯¸ì¤‘ ë¬´ì—­ì „ìŸì´ ì¥ê¸°í™”ë  ê²½ìš° êµ­ë‚´ ê²½ì œì— ìƒë‹¹í•œ ì˜í–¥ì„ ë¯¸ì¹  ê²ƒìœ¼ë¡œ ë¶„ì„í–ˆë‹¤.",
                "source": "ê²½ì œì—°êµ¬ì†Œ",
                "published": "2025-02-15T11:00:00",
            },
            {
                "title": "ì¦ì‹œ ê¸‰ë½â€¦ì½”ìŠ¤í”¼ 2% ì´ìƒ í•˜ë½ ë§ˆê°",
                "summary": "ê´€ì„¸ ì •ì±… ë°œí‘œ ì—¬íŒŒë¡œ êµ­ë‚´ ì¦ì‹œê°€ ê¸‰ë½í–ˆë‹¤. ì½”ìŠ¤í”¼ëŠ” 2% ì´ìƒ í•˜ë½í•˜ë©° íˆ¬ììë“¤ì˜ ë¶ˆì•ˆê°ì´ ì»¤ì§€ê³  ìˆë‹¤.",
                "source": "ì¦ê¶Œíƒ€ì„ìŠ¤",
                "published": "2025-02-15T15:30:00",
            },
            {
                "title": 'ì •ë¶€ "ìˆ˜ì¶œê¸°ì—… ì§€ì› ëŒ€ì±… ë§ˆë ¨ ì¤‘"',
                "summary": "ì •ë¶€ëŠ” ë¯¸êµ­ì˜ ê´€ì„¸ ì •ì±…ì— ëŒ€ì‘í•˜ì—¬ ìˆ˜ì¶œ ê¸°ì—… ì§€ì› ëŒ€ì±…ì„ ë§ˆë ¨ ì¤‘ì´ë¼ê³  ë°í˜”ë‹¤.",
                "source": "ì •ì±…ë¸Œë¦¬í•‘",
                "published": "2025-02-15T16:00:00",
            },
        ]

        print("\n" + "=" * 70)
        print("  Week 4 Day 1: Structured Analyzer Test")
        print("  Outlines + Ollama = JSON 100% ë³´ì¥")
        print("=" * 70)

        try:
            # Outlines ëª¨ë“œë¡œ í…ŒìŠ¤íŠ¸
            print("\nğŸ§ª Testing with Outlines backend...")
            result = await analyze_keyword_structured(
                keyword="íŠ¸ëŸ¼í”„ ê´€ì„¸",
                articles=test_articles,
            )

            print("\nâœ… ë¶„ì„ ì™„ë£Œ!")
            print("\nğŸ“Š ë¶„ì„ ê²°ê³¼:")
            print(f"   í‚¤ì›Œë“œ: {result.keyword}")
            print(f"   ì†ŒìŠ¤ ìˆ˜: {result.source_count}")
            print(f"   ëª¨ë¸: {result.model_version}")
            print(f"   ìƒì„± ë°©ì‹: {result.generation_method}")
            print(f"   ì¶”ë¡  ì‹œê°„: {result.inference_time_seconds:.2f}ì´ˆ")

            print("\nğŸ” í•µì‹¬ ì›ì¸:")
            print(f"   {result.analysis.main_cause}")

            print("\nğŸ“ˆ ê°ì„± ë¹„ìœ¨:")
            print(f"   ê¸ì •: {result.analysis.sentiment_ratio.positive:.0%}")
            print(f"   ë¶€ì •: {result.analysis.sentiment_ratio.negative:.0%}")
            print(f"   ì¤‘ë¦½: {result.analysis.sentiment_ratio.neutral:.0%}")

            print("\nğŸ’¬ í•µì‹¬ ì˜ê²¬:")
            for i, opinion in enumerate(result.analysis.key_opinions, 1):
                print(f"   {i}. {opinion}")

            print("\nğŸ“„ 3ì¤„ ìš”ì•½:")
            for line in result.analysis.summary.split("\n"):
                print(f"   {line}")

            print(f"\n   ìœ íš¨ì„± ê²€ì‚¬: {'âœ… í†µê³¼' if result.is_valid() else 'âŒ ì‹¤íŒ¨'}")

            # JSON ì§ë ¬í™” í…ŒìŠ¤íŠ¸
            print("\nğŸ“¦ JSON ì§ë ¬í™” í…ŒìŠ¤íŠ¸:")
            json_output = result.model_dump_json(indent=2)
            print(f"   í¬ê¸°: {len(json_output)} bytes")
            print("   âœ… JSON ì§ë ¬í™” ì„±ê³µ")

        except Exception as e:
            print(f"\nâŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            import traceback

            traceback.print_exc()

    asyncio.run(main())
