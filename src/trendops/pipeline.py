# src/trendops/pipeline.py
"""
TrendOps í†µí•© íŒŒì´í”„ë¼ì¸

Blueprint Week 2: End-to-End MVP íŒŒì´í”„ë¼ì¸
ì „ì²´ íë¦„: Trigger â†’ Collect â†’ Embed & Store â†’ Retrieve â†’ Analyze â†’ Output

âš ï¸ í•˜ë“œì›¨ì–´ ì œì•½ì‚¬í•­ (Blueprint Section 1.6):
- GPU (16GB): vLLM ë‹¨ë… ì ìœ 
- Embedding: CPU ì „ìš© (8 threads)
- ChromaDB: SQLite ë°±ì—”ë“œ (ê²½ëŸ‰)

ì‚¬ìš©ë²•:
    python -m trendops.pipeline "íŠ¸ëŸ¼í”„ ê´€ì„¸"

    ë˜ëŠ” Pythonì—ì„œ:
        from trendops.pipeline import run_pipeline
        result = await run_pipeline("íŠ¸ëŸ¼í”„ ê´€ì„¸")
"""
from __future__ import annotations

import asyncio
import time
from datetime import datetime
from typing import Any

from pydantic import BaseModel, Field

from trendops.config.settings import get_settings
from trendops.utils.logger import get_logger, setup_logging

logger = get_logger(__name__)


# =============================================================================
# Pipeline Result Models
# =============================================================================


class PipelineStageResult(BaseModel):
    """ê°œë³„ ë‹¨ê³„ ì‹¤í–‰ ê²°ê³¼"""

    stage: str = Field(..., description="ë‹¨ê³„ ì´ë¦„")
    success: bool = Field(..., description="ì„±ê³µ ì—¬ë¶€")
    duration_seconds: float = Field(..., description="ì†Œìš” ì‹œê°„")
    data: dict[str, Any] = Field(default_factory=dict, description="ë‹¨ê³„ë³„ ë°ì´í„°")
    error: str | None = Field(None, description="ì—ëŸ¬ ë©”ì‹œì§€")


class PipelineResult(BaseModel):
    """íŒŒì´í”„ë¼ì¸ ì „ì²´ ì‹¤í–‰ ê²°ê³¼"""

    keyword: str = Field(..., description="ì…ë ¥ í‚¤ì›Œë“œ")
    success: bool = Field(..., description="ì „ì²´ ì„±ê³µ ì—¬ë¶€")
    total_duration_seconds: float = Field(..., description="ì´ ì†Œìš” ì‹œê°„")
    stages: list[PipelineStageResult] = Field(default_factory=list, description="ë‹¨ê³„ë³„ ê²°ê³¼")
    analysis: dict[str, Any] | None = Field(None, description="ìµœì¢… ë¶„ì„ ê²°ê³¼")
    started_at: datetime = Field(default_factory=datetime.now, description="ì‹œì‘ ì‹œê°„")

    def get_stage(self, stage_name: str) -> PipelineStageResult | None:
        """íŠ¹ì • ë‹¨ê³„ ê²°ê³¼ ì¡°íšŒ"""
        for stage in self.stages:
            if stage.stage == stage_name:
                return stage
        return None


# =============================================================================
# Pipeline Implementation
# =============================================================================


async def run_pipeline(
    keyword: str,
    max_articles: int = 20,
    top_k_retrieve: int | None = None,
    skip_llm: bool = False,
) -> PipelineResult:
    """
    TrendOps í†µí•© íŒŒì´í”„ë¼ì¸ ì‹¤í–‰

    ì‹¤í–‰ íë¦„:
    1. [Trigger] í‚¤ì›Œë“œ ì…ë ¥ ë°›ìŒ
    2. [Collect] RSSë¡œ ë‰´ìŠ¤ ìˆ˜ì§‘
    3. [Embed & Store] ìˆ˜ì§‘ëœ ë‰´ìŠ¤ë¥¼ CPUë¡œ ì„ë² ë”©í•˜ì—¬ ChromaDBì— ì €ì¥
    4. [Retrieve] ChromaDBì—ì„œ ê´€ë ¨ë„ ë†’ì€ ë¬¸ì„œ Top K ê²€ìƒ‰ (RAG ì¤€ë¹„)
    5. [Analyze] ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ Contextë¡œ vLLMì— ë¶„ì„ ìš”ì²­
    6. [Output] ê²°ê³¼ ë°˜í™˜

    Args:
        keyword: ë¶„ì„í•  í‚¤ì›Œë“œ
        max_articles: ìˆ˜ì§‘í•  ìµœëŒ€ ê¸°ì‚¬ ìˆ˜ (ê¸°ë³¸ 20)
        top_k_retrieve: RAG ê²€ìƒ‰ ë¬¸ì„œ ìˆ˜ (Noneì´ë©´ ì„¤ì •ê°’ ì‚¬ìš©)
        skip_llm: LLM ë¶„ì„ ê±´ë„ˆë›°ê¸° (í…ŒìŠ¤íŠ¸ìš©)

    Returns:
        PipelineResult: íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ê²°ê³¼
    """
    settings = get_settings()

    if top_k_retrieve is None:
        top_k_retrieve = settings.pipeline_top_k_retrieve

    pipeline_start = time.time()
    stages: list[PipelineStageResult] = []

    logger.info("ğŸš€ Pipeline started", extra={"keyword": keyword, "max_articles": max_articles})

    # =========================================================================
    # Stage 1: Trigger (í‚¤ì›Œë“œ ì…ë ¥)
    # =========================================================================
    stage_start = time.time()

    trigger_result = PipelineStageResult(
        stage="trigger",
        success=True,
        duration_seconds=round(time.time() - stage_start, 3),
        data={"keyword": keyword, "timestamp": datetime.now().isoformat()},
    )
    stages.append(trigger_result)

    logger.info(f"âœ… [1/6] Trigger: keyword='{keyword}'")

    # =========================================================================
    # Stage 2: Collect (RSS ìˆ˜ì§‘)
    # =========================================================================
    stage_start = time.time()

    try:
        # Lazy import to avoid circular dependencies
        from trendops.collector.collector_rss_google import GoogleNewsRSSCollector

        async with GoogleNewsRSSCollector() as collector:
            collection_result = await collector.fetch(keyword, max_results=max_articles)

        if not collection_result.success or not collection_result.articles:
            raise RuntimeError(
                f"RSS collection failed: {collection_result.error_message or 'No articles found'}"
            )

        articles = collection_result.articles

        collect_result = PipelineStageResult(
            stage="collect",
            success=True,
            duration_seconds=round(time.time() - stage_start, 3),
            data={
                "source": "google_news_rss",
                "article_count": len(articles),
                "articles": [
                    {"title": a.title, "link": a.link, "published": str(a.published)}
                    for a in articles
                ],
            },
        )
        stages.append(collect_result)

        logger.info(
            f"âœ… [2/6] Collect: {len(articles)} articles from Google News RSS",
            extra={"article_count": len(articles)},
        )

    except Exception as e:
        logger.error(f"âŒ [2/6] Collect failed: {e}")
        stages.append(
            PipelineStageResult(
                stage="collect",
                success=False,
                duration_seconds=round(time.time() - stage_start, 3),
                error=str(e),
            )
        )

        return PipelineResult(
            keyword=keyword,
            success=False,
            total_duration_seconds=round(time.time() - pipeline_start, 3),
            stages=stages,
        )

    # =========================================================================
    # Stage 3: Embed & Store (CPU ì„ë² ë”© â†’ ChromaDB ì €ì¥)
    # =========================================================================
    stage_start = time.time()

    try:
        from trendops.service.embedding_service import get_embedding_service
        from trendops.store.vector_store import get_vector_store

        embedding_service = get_embedding_service()
        vector_store = get_vector_store()

        # í…ìŠ¤íŠ¸ ì¤€ë¹„ (ì œëª© + ìš”ì•½)
        texts = [
            f"{article.title}. {article.summary}" if article.summary else article.title
            for article in articles
        ]

        # ë©”íƒ€ë°ì´í„° ì¤€ë¹„
        metadatas = [
            {
                "title": article.title,
                "source": article.source,
                "keyword": keyword,
                "link": article.link,
                "published_at": article.published.isoformat() if article.published else None,
            }
            for article in articles
        ]

        # CPUì—ì„œ ë°°ì¹˜ ì„ë² ë”©
        logger.info("  â†’ Embedding on CPU...")
        embeddings = embedding_service.embed_batch(texts, show_progress=False)

        # ChromaDBì— ì €ì¥
        logger.info("  â†’ Storing in ChromaDB...")
        add_result = vector_store.add_documents(
            contents=texts,
            embeddings=embeddings,
            metadatas=metadatas,
            skip_duplicates=True,
        )

        embed_store_result = PipelineStageResult(
            stage="embed_store",
            success=True,
            duration_seconds=round(time.time() - stage_start, 3),
            data={
                "embedding_model": settings.embedding_model_name,
                "embedding_dimension": embedding_service.embedding_dimension,
                "documents_added": add_result.added_count,
                "documents_skipped": add_result.skipped_count,
                "total_in_store": vector_store.count,
            },
        )
        stages.append(embed_store_result)

        logger.info(
            f"âœ… [3/6] Embed & Store: {add_result.added_count} added, "
            f"{add_result.skipped_count} skipped (total: {vector_store.count})",
            extra={
                "added": add_result.added_count,
                "skipped": add_result.skipped_count,
            },
        )

    except Exception as e:
        logger.error(f"âŒ [3/6] Embed & Store failed: {e}")
        stages.append(
            PipelineStageResult(
                stage="embed_store",
                success=False,
                duration_seconds=round(time.time() - stage_start, 3),
                error=str(e),
            )
        )

        return PipelineResult(
            keyword=keyword,
            success=False,
            total_duration_seconds=round(time.time() - pipeline_start, 3),
            stages=stages,
        )

    # =========================================================================
    # Stage 4: Retrieve (RAG - ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰)
    # =========================================================================
    stage_start = time.time()

    try:
        # í‚¤ì›Œë“œë¥¼ ì¿¼ë¦¬ë¡œ ì„ë² ë”©
        query_embedding = embedding_service.embed(keyword)

        # ChromaDBì—ì„œ ê²€ìƒ‰
        search_results = vector_store.search(
            query_embedding=query_embedding,
            top_k=top_k_retrieve,
        )

        if not search_results:
            logger.warning("No relevant documents found, using all collected articles")
            # Fallback: ìˆ˜ì§‘ëœ ê¸°ì‚¬ ì „ì²´ ì‚¬ìš©
            retrieved_docs = [
                {"title": a.title, "content": f"{a.title}. {a.summary}", "similarity": 1.0}
                for a in articles[:top_k_retrieve]
            ]
        else:
            retrieved_docs = [
                {
                    "title": r.metadata.get("title", ""),
                    "content": r.content,
                    "similarity": r.similarity,
                    "source": r.metadata.get("source", "unknown"),
                }
                for r in search_results
            ]

        retrieve_result = PipelineStageResult(
            stage="retrieve",
            success=True,
            duration_seconds=round(time.time() - stage_start, 3),
            data={
                "top_k": top_k_retrieve,
                "retrieved_count": len(retrieved_docs),
                "documents": retrieved_docs,
            },
        )
        stages.append(retrieve_result)

        logger.info(
            f"âœ… [4/6] Retrieve: Top {len(retrieved_docs)} documents retrieved",
            extra={"retrieved_count": len(retrieved_docs)},
        )

        # Top ë¬¸ì„œ ì¶œë ¥
        for i, doc in enumerate(retrieved_docs[:3], 1):
            logger.debug(f"    #{i} [{doc['similarity']:.3f}] {doc['title'][:50]}...")

    except Exception as e:
        logger.error(f"âŒ [4/6] Retrieve failed: {e}")
        stages.append(
            PipelineStageResult(
                stage="retrieve",
                success=False,
                duration_seconds=round(time.time() - stage_start, 3),
                error=str(e),
            )
        )

        return PipelineResult(
            keyword=keyword,
            success=False,
            total_duration_seconds=round(time.time() - pipeline_start, 3),
            stages=stages,
        )

    # =========================================================================
    # Stage 5: Analyze (vLLMìœ¼ë¡œ ë¶„ì„)
    # =========================================================================
    stage_start = time.time()

    if skip_llm:
        logger.info("â­ï¸  [5/6] Analyze: Skipped (skip_llm=True)")
        stages.append(
            PipelineStageResult(
                stage="analyze",
                success=True,
                duration_seconds=0.0,
                data={"skipped": True},
            )
        )
        analysis_output = None
    else:
        try:
            from trendops.analyst.analyzer_llm import LLMAnalyzer

            # ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ LLM ë¶„ì„ìš© í˜•íƒœë¡œ ë³€í™˜
            analysis_articles = [
                {
                    "title": doc["title"],
                    "summary": doc["content"],
                    "source": doc.get("source", "unknown"),
                }
                for doc in retrieved_docs
            ]

            async with LLMAnalyzer() as analyzer:
                analysis_result = await analyzer.analyze(
                    keyword=keyword,
                    articles=analysis_articles,
                )

            analysis_output = {
                "main_cause": analysis_result.analysis.main_cause,
                "sentiment_ratio": {
                    "positive": analysis_result.analysis.sentiment_ratio.positive,
                    "negative": analysis_result.analysis.sentiment_ratio.negative,
                    "neutral": analysis_result.analysis.sentiment_ratio.neutral,
                },
                "key_opinions": analysis_result.analysis.key_opinions,
                "summary": analysis_result.analysis.summary,
                "model_version": analysis_result.model_version,
                "inference_time": analysis_result.inference_time_seconds,
                "is_valid": analysis_result.is_valid(),
            }

            analyze_stage = PipelineStageResult(
                stage="analyze",
                success=True,
                duration_seconds=round(time.time() - stage_start, 3),
                data=analysis_output,
            )
            stages.append(analyze_stage)

            logger.info(
                f"âœ… [5/6] Analyze: LLM analysis complete "
                f"(inference: {analysis_result.inference_time_seconds:.2f}s)",
                extra={"inference_time": analysis_result.inference_time_seconds},
            )

        except Exception as e:
            logger.error(f"âŒ [5/6] Analyze failed: {e}")
            stages.append(
                PipelineStageResult(
                    stage="analyze",
                    success=False,
                    duration_seconds=round(time.time() - stage_start, 3),
                    error=str(e),
                )
            )
            analysis_output = None

    # =========================================================================
    # Stage 6: Output (ê²°ê³¼ ë°˜í™˜)
    # =========================================================================
    stage_start = time.time()

    total_duration = round(time.time() - pipeline_start, 3)

    output_result = PipelineStageResult(
        stage="output",
        success=True,
        duration_seconds=round(time.time() - stage_start, 3),
        data={"total_duration": total_duration},
    )
    stages.append(output_result)

    # ìµœì¢… ê²°ê³¼ ìƒì„±
    pipeline_result = PipelineResult(
        keyword=keyword,
        success=all(s.success for s in stages),
        total_duration_seconds=total_duration,
        stages=stages,
        analysis=analysis_output,
    )

    logger.info(
        f"âœ… [6/6] Output: Pipeline complete (total: {total_duration:.2f}s)",
        extra={"total_duration": total_duration, "success": pipeline_result.success},
    )

    return pipeline_result


# =============================================================================
# Pretty Print Functions
# =============================================================================


def print_pipeline_result(result: PipelineResult) -> None:
    """íŒŒì´í”„ë¼ì¸ ê²°ê³¼ë¥¼ ì˜ˆì˜ê²Œ ì¶œë ¥"""

    print("\n" + "=" * 70)
    print("  ğŸ¯ TrendOps Pipeline Result")
    print("=" * 70)

    # ê¸°ë³¸ ì •ë³´
    status = "âœ… SUCCESS" if result.success else "âŒ FAILED"
    print(f"\n  Keyword: {result.keyword}")
    print(f"  Status: {status}")
    print(f"  Total Time: {result.total_duration_seconds:.2f}s")
    print(f"  Started: {result.started_at.strftime('%Y-%m-%d %H:%M:%S')}")

    # ë‹¨ê³„ë³„ ê²°ê³¼
    print("\n" + "-" * 70)
    print("  ğŸ“Š Stage Results")
    print("-" * 70)

    stage_names = {
        "trigger": "1. Trigger",
        "collect": "2. Collect",
        "embed_store": "3. Embed & Store",
        "retrieve": "4. Retrieve",
        "analyze": "5. Analyze",
        "output": "6. Output",
    }

    for stage in result.stages:
        name = stage_names.get(stage.stage, stage.stage)
        status_icon = "âœ…" if stage.success else "âŒ"
        print(f"  {status_icon} {name}: {stage.duration_seconds:.3f}s")

        if stage.error:
            print(f"      â””â”€ Error: {stage.error}")

    # ë¶„ì„ ê²°ê³¼ (ìˆëŠ” ê²½ìš°)
    if result.analysis:
        print("\n" + "-" * 70)
        print("  ğŸ“ Analysis Result")
        print("-" * 70)

        print("\n  ğŸ” í•µì‹¬ ì›ì¸:")
        print(f"     {result.analysis['main_cause']}")

        print("\n  ğŸ“ˆ ê°ì„± ë¹„ìœ¨:")
        sentiment = result.analysis["sentiment_ratio"]
        print(f"     ğŸŸ¢ ê¸ì •: {sentiment['positive']:.0%}")
        print(f"     ğŸ”´ ë¶€ì •: {sentiment['negative']:.0%}")
        print(f"     âšª ì¤‘ë¦½: {sentiment['neutral']:.0%}")

        print("\n  ğŸ’¬ í•µì‹¬ ì˜ê²¬:")
        for i, opinion in enumerate(result.analysis["key_opinions"], 1):
            print(f"     {i}. {opinion}")

        print("\n  ğŸ“„ 3ì¤„ ìš”ì•½:")
        for line in result.analysis["summary"].split("\n"):
            print(f"     {line}")

        print(f"\n  â„¹ï¸  Model: {result.analysis['model_version']}")
        print(f"     Inference: {result.analysis['inference_time']:.2f}s")
        print(f"     Valid: {'Yes' if result.analysis['is_valid'] else 'No'}")

    print("\n" + "=" * 70)


# =============================================================================
# CLI Entry Point
# =============================================================================


async def main(keyword: str, skip_llm: bool = False) -> None:
    """CLI ë©”ì¸ í•¨ìˆ˜"""

    # ë¡œê¹… ì„¤ì •
    setup_logging(level="INFO", enable_file=True)

    print("\n" + "=" * 70)
    print("  ğŸš€ TrendOps Pipeline")
    print("=" * 70)
    print(f"\n  Keyword: {keyword}")
    print(f"  Skip LLM: {skip_llm}")
    print("\n  Starting pipeline...\n")

    try:
        result = await run_pipeline(
            keyword=keyword,
            skip_llm=skip_llm,
        )

        print_pipeline_result(result)

    except Exception as e:
        logger.exception(f"Pipeline failed with exception: {e}")
        raise


if __name__ == "__main__":
    import sys

    # ê¸°ë³¸ í‚¤ì›Œë“œ ë˜ëŠ” ëª…ë ¹ì¤„ ì¸ì
    test_keyword = sys.argv[1] if len(sys.argv) > 1 else "íŠ¸ëŸ¼í”„ ê´€ì„¸"

    # --skip-llm ì˜µì…˜ ì²´í¬
    skip_llm = "--skip-llm" in sys.argv

    asyncio.run(main(test_keyword, skip_llm=skip_llm))
