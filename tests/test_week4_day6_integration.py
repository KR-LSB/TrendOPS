# tests/test_week4_day6_integration.py
"""
Week 4 Day 6: ì „ì²´ íŒŒì´í”„ë¼ì¸ í†µí•© í…ŒìŠ¤íŠ¸

í…ŒìŠ¤íŠ¸ ë²”ìœ„:
1. ëª¨ë“ˆ ê°„ í†µí•© ê²€ì¦
2. End-to-End ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸
3. ì„±ëŠ¥ ì¸¡ì • ë° ë²¤ì¹˜ë§ˆí¬
4. ì—ëŸ¬ ë³µêµ¬ ì‹œë‚˜ë¦¬ì˜¤
5. ë°°ì¹˜ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸
6. Week 4 ì™„ë£Œ ê²€ì¦

í†µí•© ëª¨ë“ˆ:
- Day 1: structured_analyzer.py (Outlines + Ollama)
- Day 2: schemas.py (Pydantic ìŠ¤í‚¤ë§ˆ)
- Day 3: guardrail.py (ì½˜í…ì¸  ì•ˆì „ì„±)
- Day 4: safe_pipeline.py (Self-Correction Loop)
- Day 5: error_handler.py (Circuit Breaker + Retry)

ì‹¤í–‰:
    python test_week4_day6_integration.py
"""

from __future__ import annotations

import asyncio
import json
import sys
import time
from dataclasses import dataclass, field
from datetime import datetime
from typing import Any

# =============================================================================
# Import All Week 4 Modules
# =============================================================================

try:
    # Day 2: Schemas
    from trendops.schemas import (
        # Enums
        TrendSource,
        JobStatus,
        SentimentType,
        GuardrailAction,
        GenerationMethod,
        ErrorCategory,
        ErrorSeverity,
        PipelineStage,
        GuardrailIssueType,
        # Schemas
        TrendKeyword,
        TrendJob,
        NewsArticle,
        CollectionResult,
        SentimentRatio,
        AnalysisOutput,
        AnalysisResult,
        GuardrailIssue,
        GuardrailResult,
        PipelineError,
        PipelineState,
    )
    
    # Day 3: Guardrail
    from trendops.analyst.guardrail import (
        ContentGuardrail,
        GuardrailConfig,
        RuleBasedChecker,
        check_content_safety,
    )
    
    # Day 4: Safe Pipeline
    from trendops.analyst.safe_pipeline import (
        SafeAnalysisPipeline,
        SafePipelineResult,
        PipelineStatus,
        PipelineMetrics,
        analyze_keyword_safely,
    )
    
    # Day 5: Error Handler
    from trendops.core.error_handler import (
        CircuitBreaker,
        CircuitBreakerConfig,
        CircuitBreakerError,
        CircuitState,
        RetryConfig,
        RetryResult,
        retry_async,
        ErrorClassifier,
        ErrorHandlerManager,
        with_retry,
        with_circuit_breaker,
        with_error_handling,
        get_error_manager,
    )
    
    IMPORTS_OK = True
    IMPORT_ERROR = None

except ImportError as e:
    # Fallback: ë¡œì»¬ í…ŒìŠ¤íŠ¸ìš© (ê°™ì€ ë””ë ‰í† ë¦¬ì— íŒŒì¼ì´ ìˆëŠ” ê²½ìš°)
    try:
        from schemas import (
            TrendSource, JobStatus, SentimentType, GuardrailAction,
            GenerationMethod, ErrorCategory, ErrorSeverity, PipelineStage,
            GuardrailIssueType, TrendKeyword, TrendJob, NewsArticle,
            CollectionResult, SentimentRatio, AnalysisOutput, AnalysisResult,
            GuardrailIssue, GuardrailResult, PipelineError, PipelineState,
        )
        from guardrail import (
            ContentGuardrail, GuardrailConfig, RuleBasedChecker, check_content_safety,
        )
        from safe_pipeline import (
            SafeAnalysisPipeline, SafePipelineResult, PipelineStatus,
            PipelineMetrics, analyze_keyword_safely,
        )
        from error_handler import (
            CircuitBreaker, CircuitBreakerConfig, CircuitBreakerError,
            CircuitState, RetryConfig, RetryResult, retry_async,
            ErrorClassifier, ErrorHandlerManager, with_retry,
            with_circuit_breaker, with_error_handling, get_error_manager,
        )
        IMPORTS_OK = True
        IMPORT_ERROR = None
    except ImportError as e2:
        IMPORTS_OK = False
        IMPORT_ERROR = str(e2)


# =============================================================================
# Test Data
# =============================================================================

SAMPLE_ARTICLES = [
    {
        "title": "íŠ¸ëŸ¼í”„, ì¤‘êµ­ì‚° ì œí’ˆ 25% ê´€ì„¸ ë¶€ê³¼ ë°œí‘œ",
        "summary": "ë¯¸êµ­ ëŒ€í†µë ¹ì´ ë¬´ì—­ ì „ìŸ ê²©í™” ì†ì—ì„œ ìƒˆë¡œìš´ ê´€ì„¸ ì •ì±…ì„ ë°œí‘œí–ˆë‹¤.",
        "source": "ê²½ì œì¼ë³´",
    },
    {
        "title": "êµ­ë‚´ ìˆ˜ì¶œê¸°ì—…ë“¤ 'ë¹„ìƒ'â€¦ë°˜ë„ì²´Â·ë°°í„°ë¦¬ ì—…ì¢… íƒ€ê²© ìš°ë ¤",
        "summary": "ë¯¸êµ­ì˜ ê´€ì„¸ ì •ì±… ë°œí‘œ ì´í›„ êµ­ë‚´ ìˆ˜ì¶œ ê¸°ì—…ë“¤ì´ ë¹„ìƒ ëŒ€ì‘ì— ë‚˜ì„°ë‹¤.",
        "source": "ì‚°ì—…ë‰´ìŠ¤",
    },
    {
        "title": "ì „ë¬¸ê°€ ë¶„ì„: êµ­ë‚´ GDP ì˜í–¥ ì „ë§",
        "summary": "ê²½ì œ ì „ë¬¸ê°€ë“¤ì€ ì¥ê¸°í™”ë  ê²½ìš° êµ­ë‚´ ê²½ì œì— ìƒë‹¹í•œ ì˜í–¥ì´ ìˆì„ ê²ƒìœ¼ë¡œ ë¶„ì„í–ˆë‹¤.",
        "source": "ê²½ì œì—°êµ¬ì†Œ",
    },
    {
        "title": "ì •ë¶€, ìˆ˜ì¶œê¸°ì—… ì§€ì› ëŒ€ì±… ë°œí‘œ ì˜ˆì •",
        "summary": "ì •ë¶€ëŠ” ê´€ì„¸ ì˜í–¥ì„ ìµœì†Œí™”í•˜ê¸° ìœ„í•œ ì§€ì›ì±…ì„ ë§ˆë ¨ ì¤‘ì´ë¼ê³  ë°í˜”ë‹¤.",
        "source": "ì •ì±…ë‰´ìŠ¤",
    },
    {
        "title": "ì¦ì‹œ ê¸‰ë½â€¦ì½”ìŠ¤í”¼ 2% ì´ìƒ í•˜ë½",
        "summary": "ê´€ì„¸ ì´ìŠˆë¡œ ì¸í•´ êµ­ë‚´ ì¦ì‹œê°€ ê¸‰ë½í•˜ë©° íˆ¬ììë“¤ì˜ ë¶ˆì•ˆê°ì´ ì»¤ì§€ê³  ìˆë‹¤.",
        "source": "ê¸ˆìœµë‰´ìŠ¤",
    },
]

# ë¬¸ì œ ìˆëŠ” ì½˜í…ì¸  ìƒ˜í”Œ
PROBLEMATIC_CONTENTS = {
    "political_bias": "ë¬´ëŠ¥í•œ ì •ë¶€ì˜ ìµœì•…ì˜ ì •ì±…ìœ¼ë¡œ êµ­ë¯¼ì´ ê³ í†µë°›ê³  ìˆë‹¤",
    "hate_speech": "ê·¸ ì‚¬ëŒë“¤ì€ ë‹¤ í‹€ë”±ì´ì•¼",
    "sensationalism": "ì¶©ê²©!! ëŒ€ë°• ì‚¬ê±´!! ë°œì¹µ ë’¤ì§‘í˜”ë‹¤!!!",
    "personal_info": "ì—°ë½ì²˜: 010-1234-5678ë¡œ ë¬¸ì˜í•˜ì„¸ìš”",
    "safe_content": "íŠ¸ëŸ¼í”„ ëŒ€í†µë ¹ì´ ê´€ì„¸ ì •ì±…ì„ ë°œí‘œí–ˆìŠµë‹ˆë‹¤. ì „ë¬¸ê°€ë“¤ì€ ì˜í–¥ì„ ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤.",
}


# =============================================================================
# Performance Metrics
# =============================================================================

@dataclass
class PerformanceReport:
    """ì„±ëŠ¥ ì¸¡ì • ë¦¬í¬íŠ¸"""
    test_name: str
    total_time: float = 0.0
    iterations: int = 0
    success_count: int = 0
    failure_count: int = 0
    avg_time: float = 0.0
    min_time: float = float('inf')
    max_time: float = 0.0
    details: dict[str, Any] = field(default_factory=dict)
    
    def record(self, elapsed: float, success: bool):
        self.iterations += 1
        self.total_time += elapsed
        if success:
            self.success_count += 1
        else:
            self.failure_count += 1
        self.avg_time = self.total_time / self.iterations
        self.min_time = min(self.min_time, elapsed)
        self.max_time = max(self.max_time, elapsed)
    
    def to_dict(self) -> dict:
        return {
            "test_name": self.test_name,
            "iterations": self.iterations,
            "success_rate": f"{self.success_count / self.iterations:.1%}" if self.iterations > 0 else "N/A",
            "total_time": f"{self.total_time:.2f}s",
            "avg_time": f"{self.avg_time:.3f}s",
            "min_time": f"{self.min_time:.3f}s" if self.min_time != float('inf') else "N/A",
            "max_time": f"{self.max_time:.3f}s",
        }


# =============================================================================
# Test Runner
# =============================================================================

class IntegrationTestRunner:
    """í†µí•© í…ŒìŠ¤íŠ¸ ëŸ¬ë„ˆ"""
    
    def __init__(self):
        self.passed = 0
        self.failed = 0
        self.performance_reports: list[PerformanceReport] = []
    
    def log_result(self, name: str, passed: bool, message: str = ""):
        status = "âœ… PASS" if passed else "âŒ FAIL"
        print(f"  {status}: {name}")
        if message:
            print(f"         {message}")
        if passed:
            self.passed += 1
        else:
            self.failed += 1
    
    def log_section(self, title: str):
        print(f"\n{'â”€' * 60}")
        print(f"  {title}")
        print(f"{'â”€' * 60}")
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # Test 1: Module Import Verification
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    
    async def test_module_imports(self):
        """ëª¨ë“ˆ ì„í¬íŠ¸ ê²€ì¦"""
        print("\nğŸ“¦ Test 1: Module Imports")
        print("-" * 50)
        
        self.log_result(
            "All Week 4 modules imported",
            IMPORTS_OK,
            IMPORT_ERROR if not IMPORTS_OK else "schemas, guardrail, safe_pipeline, error_handler"
        )
        
        if not IMPORTS_OK:
            return False
        
        # ìŠ¤í‚¤ë§ˆ í´ë˜ìŠ¤ ì¡´ì¬ í™•ì¸
        schema_classes = [
            SentimentRatio, AnalysisOutput, AnalysisResult,
            GuardrailIssue, GuardrailResult, PipelineError,
        ]
        all_schemas_exist = all(cls is not None for cls in schema_classes)
        self.log_result(
            "Day 2 schemas available",
            all_schemas_exist,
            f"{len(schema_classes)} schema classes"
        )
        
        # Guardrail í´ë˜ìŠ¤ í™•ì¸
        self.log_result(
            "Day 3 guardrail available",
            ContentGuardrail is not None,
        )
        
        # Pipeline í´ë˜ìŠ¤ í™•ì¸
        self.log_result(
            "Day 4 safe_pipeline available",
            SafeAnalysisPipeline is not None,
        )
        
        # Error Handler í´ë˜ìŠ¤ í™•ì¸
        self.log_result(
            "Day 5 error_handler available",
            CircuitBreaker is not None and ErrorClassifier is not None,
        )
        
        return True
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # Test 2: Schema Integration
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    
    async def test_schema_integration(self):
        """ìŠ¤í‚¤ë§ˆ í†µí•© í…ŒìŠ¤íŠ¸"""
        print("\nğŸ“‹ Test 2: Schema Integration")
        print("-" * 50)
        
        # SentimentRatio ì •ê·œí™”
        ratio = SentimentRatio(positive=0.6, negative=0.8, neutral=0.4)
        total = ratio.positive + ratio.negative + ratio.neutral
        self.log_result(
            "SentimentRatio auto-normalization",
            abs(total - 1.0) < 0.05,
            f"Total: {total:.2f}"
        )
        
        # AnalysisOutput ìƒì„± ë° ê²€ì¦
        analysis_output = AnalysisOutput(
            main_cause="íŠ¸ëŸ¼í”„ ëŒ€í†µë ¹ì˜ ê´€ì„¸ ì •ì±… ë°œí‘œë¡œ ì¸í•œ ê²½ì œì  ì˜í–¥ ìš°ë ¤",
            sentiment_ratio=SentimentRatio(positive=0.2, negative=0.5, neutral=0.3),
            key_opinions=[
                "ìˆ˜ì¶œ ê¸°ì—…ë“¤ì˜ í”¼í•´ ìš°ë ¤ í™•ì‚°",
                "ë°˜ë„ì²´ ì—…ì¢… íƒ€ê²© ì˜ˆìƒ",
                "ì •ë¶€ ëŒ€ì‘ì±… ë§ˆë ¨ ì´‰êµ¬"
            ],
            summary="íŠ¸ëŸ¼í”„ ëŒ€í†µë ¹ì´ ì¤‘êµ­ì‚° ì œí’ˆì— ê´€ì„¸ë¥¼ ë¶€ê³¼í•œë‹¤ê³  ë°œí‘œí–ˆìŠµë‹ˆë‹¤.\nêµ­ë‚´ ìˆ˜ì¶œ ê¸°ì—…ë“¤ì´ ëŒ€ì‘ì— ë‚˜ì„°ìœ¼ë©° ê²½ì œì  ì˜í–¥ì´ ìš°ë ¤ë©ë‹ˆë‹¤.\nì •ë¶€ëŠ” ì§€ì›ì±… ë§ˆë ¨ì— ë‚˜ì„œê³  ìˆìŠµë‹ˆë‹¤."
        )
        self.log_result(
            "AnalysisOutput validation",
            len(analysis_output.main_cause) >= 10,
        )
        
        # AnalysisResult ìƒì„±
        result = AnalysisResult(
            keyword="íŠ¸ëŸ¼í”„ ê´€ì„¸",
            analysis=analysis_output,
            source_count=5,
            model_version="qwen2.5:7b-instruct",
            inference_time_seconds=2.5,
            generation_method=GenerationMethod.MOCK,
        )
        self.log_result(
            "AnalysisResult creation",
            result.is_valid(),
            f"Quality score: {result.quality_score}"
        )
        
        # GuardrailResult ìƒì„±
        guardrail_result = GuardrailResult(
            content_id="test-123",
            action=GuardrailAction.PASS,
            is_safe=True,
            confidence=0.95,
            issues=[],
            original_content="í…ŒìŠ¤íŠ¸ ì½˜í…ì¸ ",
        )
        self.log_result(
            "GuardrailResult creation",
            guardrail_result.is_safe,
        )
        
        # JSON ì§ë ¬í™”/ì—­ì§ë ¬í™”
        json_str = result.model_dump_json()
        restored = AnalysisResult.model_validate_json(json_str)
        self.log_result(
            "JSON serialization round-trip",
            restored.keyword == result.keyword,
            f"{len(json_str)} bytes"
        )
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # Test 3: Guardrail Integration
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    
    async def test_guardrail_integration(self):
        """Guardrail í†µí•© í…ŒìŠ¤íŠ¸"""
        print("\nğŸ›¡ï¸ Test 3: Guardrail Integration")
        print("-" * 50)
        
        guardrail = ContentGuardrail(use_mock=True)
        
        # ì•ˆì „í•œ ì½˜í…ì¸ 
        result = await guardrail.check(
            PROBLEMATIC_CONTENTS["safe_content"],
            keyword="íŠ¸ëŸ¼í”„ ê´€ì„¸"
        )
        self.log_result(
            "Safe content passes",
            result.action == GuardrailAction.PASS,
            f"Action: {result.action.value}, Confidence: {result.confidence:.2f}"
        )
        
        # ì •ì¹˜ì  í¸í–¥
        result = await guardrail.check(PROBLEMATIC_CONTENTS["political_bias"])
        self.log_result(
            "Detects political bias",
            result.action != GuardrailAction.PASS,
            f"Action: {result.action.value}, Issues: {len(result.issues)}"
        )
        
        # í˜ì˜¤ ë°œì–¸
        result = await guardrail.check(PROBLEMATIC_CONTENTS["hate_speech"])
        self.log_result(
            "Detects hate speech",
            result.action == GuardrailAction.REJECT,
            f"Action: {result.action.value}"
        )
        
        # ì„ ì •ì  í‘œí˜„
        result = await guardrail.check(PROBLEMATIC_CONTENTS["sensationalism"])
        has_issues = len(result.issues) > 0
        self.log_result(
            "Detects sensationalism",
            has_issues,
            f"Issues: {len(result.issues)}"
        )
        
        # ê°œì¸ì •ë³´
        result = await guardrail.check(PROBLEMATIC_CONTENTS["personal_info"])
        has_personal_info = any(
            i.issue_type == GuardrailIssueType.PERSONAL_INFO 
            for i in result.issues
        )
        self.log_result(
            "Detects personal info",
            has_personal_info,
        )
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # Test 4: Safe Pipeline End-to-End
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    
    async def test_safe_pipeline_e2e(self):
        """Safe Pipeline End-to-End í…ŒìŠ¤íŠ¸"""
        print("\nğŸ”„ Test 4: Safe Pipeline E2E")
        print("-" * 50)
        
        report = PerformanceReport(test_name="safe_pipeline_e2e")
        
        async with SafeAnalysisPipeline(use_mock=True) as pipeline:
            # ì •ìƒ ë¶„ì„ íë¦„
            start = time.time()
            result = await pipeline.analyze_safely(
                keyword="íŠ¸ëŸ¼í”„ ê´€ì„¸",
                articles=SAMPLE_ARTICLES,
            )
            elapsed = time.time() - start
            report.record(elapsed, result.success)
            
            self.log_result(
                "Normal analysis flow",
                result.success,
                f"Status: {result.status.value}, Time: {elapsed:.2f}s"
            )
            
            # ë¶„ì„ ê²°ê³¼ ìœ íš¨ì„±
            if result.analysis:
                self.log_result(
                    "Analysis result valid",
                    result.analysis.is_valid(),
                    f"Quality: {result.analysis.quality_score}"
                )
            
            # Guardrail ê²°ê³¼ ì¡´ì¬
            self.log_result(
                "Guardrail result present",
                result.guardrail_result is not None,
            )
            
            # ë©”íŠ¸ë¦­ìŠ¤ ìˆ˜ì§‘
            metrics = pipeline.get_metrics()
            self.log_result(
                "Metrics collected",
                metrics["total_attempts"] >= 1,
                f"Attempts: {metrics['total_attempts']}, Pass rate: {metrics['guardrail_pass_rate']:.1%}"
            )
        
        self.performance_reports.append(report)
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # Test 5: Error Handler Integration
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    
    async def test_error_handler_integration(self):
        """ì—ëŸ¬ í•¸ë“¤ëŸ¬ í†µí•© í…ŒìŠ¤íŠ¸"""
        print("\nâš ï¸ Test 5: Error Handler Integration")
        print("-" * 50)
        
        # Circuit Breaker + Retry ì¡°í•©
        breaker = CircuitBreaker(
            name="integration_test",
            config=CircuitBreakerConfig(
                failure_threshold=3,
                timeout_seconds=1.0,
            ),
        )
        
        call_count = 0
        
        @with_error_handling(
            stage="integration",
            retry_config=RetryConfig(max_attempts=2, backoff_base=0.01),
            circuit_breaker=breaker,
        )
        async def flaky_operation(succeed: bool):
            nonlocal call_count
            call_count += 1
            if not succeed:
                raise ConnectionError("Simulated failure")
            return "success"
        
        # ì„±ê³µ ì¼€ì´ìŠ¤
        result = await flaky_operation(True)
        self.log_result(
            "Error handling: success case",
            result == "success",
        )
        
        # ì‹¤íŒ¨ ì¼€ì´ìŠ¤ (ì¬ì‹œë„)
        call_count = 0
        try:
            await flaky_operation(False)
            self.log_result("Error handling: retry on failure", False)
        except ConnectionError:
            self.log_result(
                "Error handling: retry exhausted",
                call_count >= 2,
                f"Attempts: {call_count}"
            )
        
        # Circuit Breaker ë™ì‘
        self.log_result(
            "Circuit breaker records failures",
            breaker.failure_count >= 1,
            f"Failures: {breaker.failure_count}"
        )
        
        # Error Manager
        manager = get_error_manager()
        error = PipelineError(
            category=ErrorCategory.NETWORK,
            message="Integration test error",
            stage="integration",
        )
        await manager.report_error(error)
        
        summary = manager.get_error_summary()
        self.log_result(
            "Error manager integration",
            summary["total"] >= 1,
        )
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # Test 6: Batch Processing
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    
    async def test_batch_processing(self):
        """ë°°ì¹˜ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸"""
        print("\nğŸ“¦ Test 6: Batch Processing")
        print("-" * 50)
        
        report = PerformanceReport(test_name="batch_processing")
        
        keywords = ["íŠ¸ëŸ¼í”„ ê´€ì„¸", "AI ê¸°ìˆ ", "ë°˜ë„ì²´ ì‹œì¥", "ê²½ì œ ì „ë§", "ì£¼ì‹ ì‹œì¥"]
        batch_items = [(kw, SAMPLE_ARTICLES[:3]) for kw in keywords]
        
        async with SafeAnalysisPipeline(use_mock=True) as pipeline:
            start = time.time()
            results = await pipeline.analyze_batch(batch_items, concurrency=3)
            total_elapsed = time.time() - start
            
            success_count = sum(1 for r in results if r.success)
            
            for r in results:
                report.record(r.total_time_seconds, r.success)
            
            self.log_result(
                "Batch processing completes",
                len(results) == len(keywords),
                f"Processed: {len(results)}/{len(keywords)}"
            )
            
            self.log_result(
                "Batch success rate",
                success_count == len(keywords),
                f"Success: {success_count}/{len(keywords)}"
            )
            
            avg_time = sum(r.total_time_seconds for r in results) / len(results)
            self.log_result(
                "Batch performance",
                total_elapsed < len(keywords) * 2,  # ë³‘ë ¬ ì²˜ë¦¬ë¡œ ê°œë³„ í•©ë³´ë‹¤ ë¹¨ë¼ì•¼ í•¨
                f"Total: {total_elapsed:.2f}s, Avg: {avg_time:.2f}s"
            )
        
        self.performance_reports.append(report)
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # Test 7: Pipeline State Tracking
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    
    async def test_pipeline_state_tracking(self):
        """íŒŒì´í”„ë¼ì¸ ìƒíƒœ ì¶”ì  í…ŒìŠ¤íŠ¸"""
        print("\nğŸ“Š Test 7: Pipeline State Tracking")
        print("-" * 50)
        
        async with SafeAnalysisPipeline(use_mock=True) as pipeline:
            # ì—¬ëŸ¬ ë²ˆ ì‹¤í–‰
            for i in range(5):
                await pipeline.analyze_safely(f"í‚¤ì›Œë“œ{i}", SAMPLE_ARTICLES[:2])
            
            metrics = pipeline.get_metrics()
            
            self.log_result(
                "Tracks total attempts",
                metrics["total_attempts"] == 5,
                f"Total: {metrics['total_attempts']}"
            )
            
            self.log_result(
                "Calculates pass rate",
                0 <= metrics["guardrail_pass_rate"] <= 1,
                f"Rate: {metrics['guardrail_pass_rate']:.1%}"
            )
            
            self.log_result(
                "Records average time",
                metrics["avg_time_seconds"] > 0,
                f"Avg: {metrics['avg_time_seconds']:.2f}s"
            )
            
            # ë¦¬ì…‹ í…ŒìŠ¤íŠ¸
            pipeline.reset_metrics()
            reset_metrics = pipeline.get_metrics()
            self.log_result(
                "Metrics reset works",
                reset_metrics["total_attempts"] == 0,
            )
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # Test 8: Full Integration Scenario
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    
    async def test_full_integration_scenario(self):
        """ì „ì²´ í†µí•© ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸"""
        print("\nğŸ¯ Test 8: Full Integration Scenario")
        print("-" * 50)
        
        # ì‹œë‚˜ë¦¬ì˜¤: ì‹¤ì œ íŒŒì´í”„ë¼ì¸ íë¦„ ì‹œë®¬ë ˆì´ì…˜
        
        # 1. TrendKeyword ìƒì„±
        trend = TrendKeyword(
            keyword="íŠ¸ëŸ¼í”„ ê´€ì„¸",
            source=TrendSource.GOOGLE,
            trend_score=8.5,
        )
        self.log_result(
            "Step 1: TrendKeyword created",
            trend.keyword == "íŠ¸ëŸ¼í”„ ê´€ì„¸",
            f"Score: {trend.trend_score}"
        )
        
        # 2. CollectionResult ìƒì„±
        articles = [
            NewsArticle(
                title=a["title"],
                link=f"https://example.com/{i}",
                summary=a["summary"],
                source=a["source"],
            )
            for i, a in enumerate(SAMPLE_ARTICLES)
        ]
        collection = CollectionResult(
            keyword=trend.keyword,
            articles=articles,
            source=TrendSource.GOOGLE,
        )
        self.log_result(
            "Step 2: CollectionResult created",
            collection.total_count == len(SAMPLE_ARTICLES),
            f"Articles: {collection.total_count}"
        )
        
        # 3. SafeAnalysisPipeline ì‹¤í–‰
        async with SafeAnalysisPipeline(use_mock=True) as pipeline:
            article_dicts = [
                {"title": a.title, "summary": a.summary or "", "source": a.source}
                for a in collection.articles
            ]
            
            result = await pipeline.analyze_safely(
                keyword=trend.keyword,
                articles=article_dicts,
            )
            
            self.log_result(
                "Step 3: Analysis completed",
                result.success,
                f"Status: {result.status.value}"
            )
            
            # 4. ê²°ê³¼ ê²€ì¦
            if result.analysis:
                self.log_result(
                    "Step 4: Analysis valid",
                    result.analysis.is_valid(),
                    f"Main cause: {result.analysis.analysis.main_cause[:40]}..."
                )
            
            # 5. Guardrail í†µê³¼ í™•ì¸
            if result.guardrail_result:
                self.log_result(
                    "Step 5: Guardrail passed",
                    result.guardrail_result.is_safe,
                    f"Action: {result.guardrail_result.action.value}"
                )
            
            # 6. ìµœì¢… ìš”ì•½ ì¶”ì¶œ
            final_summary = result.get_final_summary()
            self.log_result(
                "Step 6: Final summary available",
                final_summary is not None and len(final_summary) > 0,
                f"Length: {len(final_summary) if final_summary else 0}"
            )
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # Test 9: Error Recovery Scenario
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    
    async def test_error_recovery_scenario(self):
        """ì—ëŸ¬ ë³µêµ¬ ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸"""
        print("\nğŸ”§ Test 9: Error Recovery Scenario")
        print("-" * 50)
        
        manager = ErrorHandlerManager()
        
        # ì„œë¹„ìŠ¤ë³„ Circuit Breaker ë“±ë¡
        ollama_breaker = manager.register_breaker(
            "ollama",
            config=CircuitBreakerConfig(failure_threshold=3, timeout_seconds=1.0)
        )
        redis_breaker = manager.register_breaker(
            "redis",
            config=CircuitBreakerConfig(failure_threshold=5, timeout_seconds=2.0)
        )
        
        # ì‹¤íŒ¨ ì‹œë®¬ë ˆì´ì…˜
        for i in range(3):
            ollama_breaker.record_failure(TimeoutError(f"Timeout {i}"))
        
        self.log_result(
            "Circuit breaker opens on failures",
            ollama_breaker.state == CircuitState.OPEN,
            f"State: {ollama_breaker.state.value}"
        )
        
        # íƒ€ì„ì•„ì›ƒ í›„ ë³µêµ¬
        await asyncio.sleep(1.1)
        
        self.log_result(
            "Circuit breaker transitions to HALF_OPEN",
            ollama_breaker.state == CircuitState.HALF_OPEN,
            f"State: {ollama_breaker.state.value}"
        )
        
        # ì„±ê³µìœ¼ë¡œ ë³µêµ¬
        ollama_breaker.record_success()
        ollama_breaker.record_success()
        
        self.log_result(
            "Circuit breaker recovers to CLOSED",
            ollama_breaker.state == CircuitState.CLOSED,
            f"State: {ollama_breaker.state.value}"
        )
        
        # ì „ì²´ ìƒíƒœ ì¡°íšŒ
        all_stats = manager.get_all_breaker_stats()
        self.log_result(
            "Manager tracks all breakers",
            len(all_stats) == 2,
            f"Breakers: {list(all_stats.keys())}"
        )
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # Test 10: Performance Benchmark
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    
    async def test_performance_benchmark(self):
        """ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸"""
        print("\nâš¡ Test 10: Performance Benchmark")
        print("-" * 50)
        
        report = PerformanceReport(test_name="performance_benchmark")
        iterations = 10
        
        async with SafeAnalysisPipeline(use_mock=True) as pipeline:
            for i in range(iterations):
                start = time.time()
                result = await pipeline.analyze_safely(
                    keyword=f"ë²¤ì¹˜ë§ˆí¬_{i}",
                    articles=SAMPLE_ARTICLES[:3],
                )
                elapsed = time.time() - start
                report.record(elapsed, result.success)
        
        self.performance_reports.append(report)
        
        self.log_result(
            f"Completed {iterations} iterations",
            report.iterations == iterations,
        )
        
        self.log_result(
            "All iterations successful",
            report.success_count == iterations,
            f"Success: {report.success_count}/{iterations}"
        )
        
        self.log_result(
            "Average time reasonable",
            report.avg_time < 2.0,  # Mock ëª¨ë“œì—ì„œ 2ì´ˆ ë¯¸ë§Œ ì˜ˆìƒ
            f"Avg: {report.avg_time:.3f}s"
        )
        
        self.log_result(
            "Time variance acceptable",
            (report.max_time - report.min_time) < 1.0,
            f"Range: {report.min_time:.3f}s - {report.max_time:.3f}s"
        )
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # Generate Report
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    
    def generate_report(self) -> dict:
        """ìµœì¢… ë¦¬í¬íŠ¸ ìƒì„±"""
        return {
            "week": 4,
            "date": datetime.now().isoformat(),
            "tests": {
                "passed": self.passed,
                "failed": self.failed,
                "total": self.passed + self.failed,
                "success_rate": f"{self.passed / (self.passed + self.failed):.1%}" if (self.passed + self.failed) > 0 else "N/A",
            },
            "performance": [r.to_dict() for r in self.performance_reports],
            "modules": {
                "day1": "structured_analyzer.py",
                "day2": "schemas.py",
                "day3": "guardrail.py",
                "day4": "safe_pipeline.py",
                "day5": "error_handler.py",
            },
        }
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # Run All Tests
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    
    async def run_all_tests(self):
        """ëª¨ë“  í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        print("\n" + "=" * 70)
        print("  Week 4 Day 6: Integration Tests")
        print("  TrendOps LLM Pipeline - Full Integration")
        print("=" * 70)
        
        # ì„í¬íŠ¸ í™•ì¸
        imports_ok = await self.test_module_imports()
        if not imports_ok:
            print("\nâŒ Module imports failed. Cannot continue.")
            return False
        
        # ëª¨ë“  í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        await self.test_schema_integration()
        await self.test_guardrail_integration()
        await self.test_safe_pipeline_e2e()
        await self.test_error_handler_integration()
        await self.test_batch_processing()
        await self.test_pipeline_state_tracking()
        await self.test_full_integration_scenario()
        await self.test_error_recovery_scenario()
        await self.test_performance_benchmark()
        
        # ìµœì¢… ë¦¬í¬íŠ¸
        report = self.generate_report()
        
        print("\n" + "=" * 70)
        print("  ğŸ“Š Week 4 Integration Test Summary")
        print("=" * 70)
        print(f"  âœ… Passed: {self.passed}")
        print(f"  âŒ Failed: {self.failed}")
        print(f"  ğŸ“ˆ Success Rate: {report['tests']['success_rate']}")
        
        print("\n  âš¡ Performance Summary:")
        for perf in self.performance_reports:
            p = perf.to_dict()
            print(f"     - {p['test_name']}: {p['avg_time']} avg, {p['success_rate']} success")
        
        print("\n  ğŸ“¦ Integrated Modules:")
        for day, module in report['modules'].items():
            print(f"     - {day}: {module}")
        
        print("\n" + "=" * 70)
        
        if self.failed == 0:
            print("  ğŸ‰ Week 4 Integration Tests PASSED!")
            print("     All modules working together correctly.")
        else:
            print("  âš ï¸ Some tests failed. Review the results above.")
        
        print("=" * 70)
        
        return self.failed == 0


# =============================================================================
# Main
# =============================================================================

async def main():
    runner = IntegrationTestRunner()
    success = await runner.run_all_tests()
    sys.exit(0 if success else 1)


if __name__ == "__main__":
    asyncio.run(main())