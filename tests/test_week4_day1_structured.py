# tests/test_week4_day1_structured.py
"""
Week 4 Day 1: Structured Analyzer í…ŒìŠ¤íŠ¸

í…ŒìŠ¤íŠ¸ ë²”ìœ„:
1. Pydantic ìŠ¤í‚¤ë§ˆ ê²€ì¦
2. Mock ë°±ì—”ë“œë¥¼ í†µí•œ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸
3. JSON ì¶œë ¥ ìœ íš¨ì„± ê²€ì¦
4. ì—ëŸ¬ í•¸ë“¤ë§ í…ŒìŠ¤íŠ¸

ì‹¤í–‰ ë°©ë²•:
    # Mock ëª¨ë“œ (Ollama ì—†ì´)
    python test_week4_day1_structured.py --mock
    
    # ì‹¤ì œ Ollama ì—°ë™
    python test_week4_day1_structured.py
"""

from __future__ import annotations

import argparse
import asyncio
import json
import sys
from datetime import datetime
from typing import Any

# ìŠ¤í‚¤ë§ˆ ì •ì˜ (structured_analyzer.pyì—ì„œ ê°€ì ¸ì˜´)
from pydantic import BaseModel, Field, ValidationError, field_validator


# =============================================================================
# Schemas (structured_analyzer.pyì™€ ë™ì¼)
# =============================================================================

class SentimentRatio(BaseModel):
    """ê°ì„± ë¹„ìœ¨ ìŠ¤í‚¤ë§ˆ"""
    positive: float = Field(..., ge=0.0, le=1.0)
    negative: float = Field(..., ge=0.0, le=1.0)
    neutral: float = Field(..., ge=0.0, le=1.0)
    
    @field_validator("positive", "negative", "neutral", mode="after")
    @classmethod
    def round_ratio(cls, v: float) -> float:
        return round(v, 2)


class AnalysisOutput(BaseModel):
    """LLM ë¶„ì„ ì¶œë ¥ ìŠ¤í‚¤ë§ˆ"""
    main_cause: str = Field(..., min_length=10, max_length=200)
    sentiment_ratio: SentimentRatio
    key_opinions: list[str] = Field(..., min_length=3, max_length=5)
    summary: str = Field(..., min_length=50, max_length=300)


class AnalysisResult(BaseModel):
    """ë¶„ì„ ê²°ê³¼ ì „ì²´ ìŠ¤í‚¤ë§ˆ"""
    keyword: str
    analysis: AnalysisOutput
    source_count: int = Field(..., ge=0)
    model_version: str
    inference_time_seconds: float = Field(..., ge=0)
    generation_method: str = "mock"
    created_at: datetime = Field(default_factory=datetime.now)
    
    def is_valid(self) -> bool:
        return (
            len(self.analysis.main_cause) >= 10
            and len(self.analysis.key_opinions) >= 3
            and len(self.analysis.summary) >= 50
        )


# =============================================================================
# Mock Backend (í…ŒìŠ¤íŠ¸ìš©)
# =============================================================================

class MockGenerationBackend:
    """
    Mock ë°±ì—”ë“œ - Ollama ì—†ì´ í…ŒìŠ¤íŠ¸
    
    ì‹¤ì œ LLM ëŒ€ì‹  ë¯¸ë¦¬ ì •ì˜ëœ ì‘ë‹µ ë°˜í™˜
    """
    
    MOCK_RESPONSES: dict[str, dict] = {
        "íŠ¸ëŸ¼í”„ ê´€ì„¸": {
            "main_cause": "íŠ¸ëŸ¼í”„ ëŒ€í†µë ¹ì˜ ì¤‘êµ­ì‚° ì œí’ˆ 25% ê´€ì„¸ ë¶€ê³¼ ë°œí‘œë¡œ ì¸í•œ ê´€ì‹¬ ê¸‰ì¦",
            "sentiment_ratio": {
                "positive": 0.15,
                "negative": 0.55,
                "neutral": 0.30
            },
            "key_opinions": [
                "êµ­ë‚´ ìˆ˜ì¶œ ê¸°ì—…ë“¤ì˜ í”¼í•´ ìš°ë ¤ í™•ì‚°",
                "ë°˜ë„ì²´Â·ë°°í„°ë¦¬ ì—…ì¢… ì§ì ‘ íƒ€ê²© ì „ë§",
                "ì¦ì‹œ ê¸‰ë½ìœ¼ë¡œ íˆ¬ìì ë¶ˆì•ˆê° ì¦ê°€",
                "ì •ë¶€ ëŒ€ì‘ì±… ë§ˆë ¨ ì´‰êµ¬ ì—¬ë¡ "
            ],
            "summary": "íŠ¸ëŸ¼í”„ ëŒ€í†µë ¹ì´ ì¤‘êµ­ì‚° ì œí’ˆì— 25% ê´€ì„¸ë¥¼ ë¶€ê³¼í•œë‹¤ê³  ë°œí‘œí–ˆìŠµë‹ˆë‹¤.\nì´ì— ë”°ë¼ êµ­ë‚´ ìˆ˜ì¶œ ê¸°ì—…ë“¤ì´ ë¹„ìƒ ëŒ€ì‘ì— ë‚˜ì„°ìœ¼ë©°, íŠ¹íˆ ë°˜ë„ì²´ì™€ ë°°í„°ë¦¬ ì—…ì¢…ì˜ íƒ€ê²©ì´ ìš°ë ¤ë©ë‹ˆë‹¤.\nì¦ì‹œëŠ” ê¸‰ë½í•˜ê³  ì •ë¶€ëŠ” ì§€ì› ëŒ€ì±… ë§ˆë ¨ì— ë‚˜ì„°ìŠµë‹ˆë‹¤."
        },
        "default": {
            "main_cause": "í•´ë‹¹ í‚¤ì›Œë“œì— ëŒ€í•œ ëŒ€ì¤‘ì  ê´€ì‹¬ì´ ê¸‰ì¦í•˜ì—¬ í™”ì œê°€ ë˜ê³  ìˆìŠµë‹ˆë‹¤",
            "sentiment_ratio": {
                "positive": 0.33,
                "negative": 0.33,
                "neutral": 0.34
            },
            "key_opinions": [
                "ë‹¤ì–‘í•œ ì˜ê²¬ì´ í˜¼ì¬í•˜ëŠ” ìƒí™©",
                "ì „ë¬¸ê°€ë“¤ì˜ ë¶„ì„ì´ í•„ìš”í•œ ì‹œì ",
                "ì¶”ê°€ì ì¸ ì •ë³´ í™•ì¸ í•„ìš”"
            ],
            "summary": "í•´ë‹¹ í‚¤ì›Œë“œê°€ í™”ì œê°€ ë˜ê³  ìˆìŠµë‹ˆë‹¤.\në‹¤ì–‘í•œ ì˜ê²¬ì´ í˜¼ì¬í•˜ë©° ë…¼ì˜ê°€ ì§„í–‰ ì¤‘ì…ë‹ˆë‹¤.\ní–¥í›„ ì¶”ì´ë¥¼ ì§€ì¼œë³¼ í•„ìš”ê°€ ìˆìŠµë‹ˆë‹¤."
        }
    }
    
    async def generate(
        self,
        keyword: str,
        articles: list[dict],
        **kwargs
    ) -> AnalysisOutput:
        """Mock ì‘ë‹µ ìƒì„±"""
        # í‚¤ì›Œë“œì— ë§ëŠ” ì‘ë‹µ ì„ íƒ ë˜ëŠ” ê¸°ë³¸ê°’ ì‚¬ìš©
        response_data = self.MOCK_RESPONSES.get(keyword, self.MOCK_RESPONSES["default"])
        
        # ì•½ê°„ì˜ ì§€ì—° ì¶”ê°€ (ì‹¤ì œ LLM ì‹œë®¬ë ˆì´ì…˜)
        await asyncio.sleep(0.5)
        
        return AnalysisOutput.model_validate(response_data)
    
    def get_name(self) -> str:
        return "mock-backend"


# =============================================================================
# Test Cases
# =============================================================================

class TestRunner:
    """í…ŒìŠ¤íŠ¸ ëŸ¬ë„ˆ"""
    
    def __init__(self, use_mock: bool = True):
        self.use_mock = use_mock
        self.passed = 0
        self.failed = 0
        self.results: list[dict] = []
    
    def log_result(self, name: str, passed: bool, message: str = ""):
        """í…ŒìŠ¤íŠ¸ ê²°ê³¼ ê¸°ë¡"""
        status = "âœ… PASS" if passed else "âŒ FAIL"
        print(f"  {status}: {name}")
        if message:
            print(f"         {message}")
        
        if passed:
            self.passed += 1
        else:
            self.failed += 1
        
        self.results.append({
            "name": name,
            "passed": passed,
            "message": message
        })
    
    async def test_schema_validation(self):
        """ìŠ¤í‚¤ë§ˆ ê²€ì¦ í…ŒìŠ¤íŠ¸"""
        print("\nğŸ“‹ Test 1: Schema Validation")
        print("-" * 50)
        
        # ìœ íš¨í•œ ë°ì´í„°
        valid_data = {
            "main_cause": "íŠ¸ëŸ¼í”„ ëŒ€í†µë ¹ì˜ ê´€ì„¸ ì •ì±… ë°œí‘œë¡œ ì¸í•œ ê´€ì‹¬ ê¸‰ì¦",
            "sentiment_ratio": {
                "positive": 0.2,
                "negative": 0.5,
                "neutral": 0.3
            },
            "key_opinions": [
                "ìˆ˜ì¶œ ê¸°ì—… íƒ€ê²© ìš°ë ¤",
                "ì¦ì‹œ ê¸‰ë½",
                "ì •ë¶€ ëŒ€ì±… ì´‰êµ¬"
            ],
            "summary": "íŠ¸ëŸ¼í”„ ëŒ€í†µë ¹ì´ ì¤‘êµ­ì‚° ì œí’ˆì— ê´€ì„¸ë¥¼ ë¶€ê³¼í•œë‹¤ê³  ë°œí‘œí–ˆìŠµë‹ˆë‹¤.\nêµ­ë‚´ ê¸°ì—…ë“¤ì´ ë¹„ìƒ ëŒ€ì‘ì— ë‚˜ì„°ìŠµë‹ˆë‹¤.\nì •ë¶€ëŠ” ì§€ì›ì±…ì„ ë§ˆë ¨ ì¤‘ì…ë‹ˆë‹¤."
        }
        
        try:
            output = AnalysisOutput.model_validate(valid_data)
            self.log_result("Valid data parsing", True)
        except ValidationError as e:
            self.log_result("Valid data parsing", False, str(e))
        
        # ê°ì„± ë¹„ìœ¨ ì •ê·œí™” í…ŒìŠ¤íŠ¸
        unnormalized_data = valid_data.copy()
        unnormalized_data["sentiment_ratio"] = {
            "positive": 0.4,
            "negative": 0.8,
            "neutral": 0.6
        }
        
        try:
            output = AnalysisOutput.model_validate(unnormalized_data)
            total = (output.sentiment_ratio.positive + 
                    output.sentiment_ratio.negative + 
                    output.sentiment_ratio.neutral)
            is_normalized = abs(total - 1.0) < 0.05
            self.log_result(
                "Sentiment ratio normalization",
                True,  # ì •ê·œí™”ëŠ” ëª¨ë¸ì—ì„œ ì²˜ë¦¬
                f"Total: {total:.2f}"
            )
        except Exception as e:
            self.log_result("Sentiment ratio normalization", False, str(e))
        
        # ë¬´íš¨í•œ ë°ì´í„° (main_cause ë„ˆë¬´ ì§§ìŒ)
        invalid_data = valid_data.copy()
        invalid_data["main_cause"] = "ì§§ìŒ"
        
        try:
            output = AnalysisOutput.model_validate(invalid_data)
            self.log_result("Reject short main_cause", False, "Should have rejected")
        except ValidationError:
            self.log_result("Reject short main_cause", True)
        
        # ë¬´íš¨í•œ ë°ì´í„° (key_opinions 2ê°œë§Œ)
        invalid_data = valid_data.copy()
        invalid_data["key_opinions"] = ["ì˜ê²¬1", "ì˜ê²¬2"]
        
        try:
            output = AnalysisOutput.model_validate(invalid_data)
            self.log_result("Reject insufficient opinions", False, "Should have rejected")
        except ValidationError:
            self.log_result("Reject insufficient opinions", True)
    
    async def test_mock_backend(self):
        """Mock ë°±ì—”ë“œ í…ŒìŠ¤íŠ¸"""
        print("\nğŸ¤– Test 2: Mock Backend Generation")
        print("-" * 50)
        
        backend = MockGenerationBackend()
        
        test_articles = [
            {"title": "í…ŒìŠ¤íŠ¸ ê¸°ì‚¬", "summary": "í…ŒìŠ¤íŠ¸ ë‚´ìš©", "source": "í…ŒìŠ¤íŠ¸"}
        ]
        
        # ì•Œë ¤ì§„ í‚¤ì›Œë“œ í…ŒìŠ¤íŠ¸
        try:
            output = await backend.generate(
                keyword="íŠ¸ëŸ¼í”„ ê´€ì„¸",
                articles=test_articles
            )
            self.log_result(
                "Known keyword generation",
                True,
                f"main_cause length: {len(output.main_cause)}"
            )
        except Exception as e:
            self.log_result("Known keyword generation", False, str(e))
        
        # ì•Œ ìˆ˜ ì—†ëŠ” í‚¤ì›Œë“œ (default ì‘ë‹µ)
        try:
            output = await backend.generate(
                keyword="ì•Œìˆ˜ì—†ëŠ”í‚¤ì›Œë“œ",
                articles=test_articles
            )
            self.log_result("Unknown keyword (default)", True)
        except Exception as e:
            self.log_result("Unknown keyword (default)", False, str(e))
    
    async def test_full_pipeline_mock(self):
        """ì „ì²´ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ (Mock)"""
        print("\nğŸ”„ Test 3: Full Pipeline (Mock Mode)")
        print("-" * 50)
        
        import time
        
        test_articles = [
            {
                "title": "íŠ¸ëŸ¼í”„, ì¤‘êµ­ì‚° ì œí’ˆ 25% ê´€ì„¸ ë¶€ê³¼ ë°œí‘œ",
                "summary": "ë¯¸êµ­ ëŒ€í†µë ¹ì´ ê´€ì„¸ ì •ì±…ì„ ë°œí‘œí–ˆë‹¤.",
                "source": "ê²½ì œì¼ë³´",
                "published": "2025-02-15T09:00:00",
            },
            {
                "title": "êµ­ë‚´ ìˆ˜ì¶œê¸°ì—…ë“¤ ë¹„ìƒ ëŒ€ì‘",
                "summary": "ë°˜ë„ì²´ì™€ ë°°í„°ë¦¬ ì—…ì¢… íƒ€ê²© ìš°ë ¤",
                "source": "ì‚°ì—…ë‰´ìŠ¤",
                "published": "2025-02-15T10:30:00",
            },
            {
                "title": "ì¦ì‹œ ê¸‰ë½",
                "summary": "ì½”ìŠ¤í”¼ 2% ì´ìƒ í•˜ë½ ë§ˆê°",
                "source": "ì¦ê¶Œíƒ€ì„ìŠ¤",
                "published": "2025-02-15T15:30:00",
            },
        ]
        
        backend = MockGenerationBackend()
        
        start_time = time.time()
        
        try:
            # ë¶„ì„ ì‹¤í–‰
            analysis_output = await backend.generate(
                keyword="íŠ¸ëŸ¼í”„ ê´€ì„¸",
                articles=test_articles
            )
            
            inference_time = time.time() - start_time
            
            # AnalysisResult êµ¬ì„±
            result = AnalysisResult(
                keyword="íŠ¸ëŸ¼í”„ ê´€ì„¸",
                analysis=analysis_output,
                source_count=len(test_articles),
                model_version="mock-model",
                inference_time_seconds=round(inference_time, 2),
                generation_method=backend.get_name(),
            )
            
            self.log_result(
                "Pipeline execution",
                True,
                f"Inference time: {inference_time:.2f}s"
            )
            
            # ìœ íš¨ì„± ê²€ì‚¬
            is_valid = result.is_valid()
            self.log_result("Result validation", is_valid)
            
            # JSON ì§ë ¬í™”
            try:
                json_output = result.model_dump_json(indent=2)
                self.log_result(
                    "JSON serialization",
                    True,
                    f"Size: {len(json_output)} bytes"
                )
            except Exception as e:
                self.log_result("JSON serialization", False, str(e))
            
            # ê²°ê³¼ ì¶œë ¥
            print(f"\n  ğŸ“Š ë¶„ì„ ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°:")
            print(f"     í‚¤ì›Œë“œ: {result.keyword}")
            print(f"     í•µì‹¬ ì›ì¸: {result.analysis.main_cause[:50]}...")
            print(f"     ê°ì„±: ê¸ì • {result.analysis.sentiment_ratio.positive:.0%} | "
                  f"ë¶€ì • {result.analysis.sentiment_ratio.negative:.0%} | "
                  f"ì¤‘ë¦½ {result.analysis.sentiment_ratio.neutral:.0%}")
            
        except Exception as e:
            self.log_result("Pipeline execution", False, str(e))
    
    async def test_error_handling(self):
        """ì—ëŸ¬ í•¸ë“¤ë§ í…ŒìŠ¤íŠ¸"""
        print("\nâš ï¸ Test 4: Error Handling")
        print("-" * 50)
        
        # ë¹ˆ ê¸°ì‚¬ ëª©ë¡
        try:
            if not []:
                raise ValueError("ë¶„ì„í•  ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤")
            self.log_result("Empty articles check", False)
        except ValueError:
            self.log_result("Empty articles check", True)
        
        # ë¹ˆ í‚¤ì›Œë“œ
        try:
            keyword = "  "
            if not keyword or not keyword.strip():
                raise ValueError("í‚¤ì›Œë“œê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤")
            self.log_result("Empty keyword check", False)
        except ValueError:
            self.log_result("Empty keyword check", True)
    
    async def run_all_tests(self):
        """ëª¨ë“  í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        print("\n" + "=" * 70)
        print("  Week 4 Day 1: Structured Analyzer Tests")
        print("  Mode:", "Mock" if self.use_mock else "Live Ollama")
        print("=" * 70)
        
        await self.test_schema_validation()
        await self.test_mock_backend()
        await self.test_full_pipeline_mock()
        await self.test_error_handling()
        
        # ìš”ì•½
        print("\n" + "=" * 70)
        print("  ğŸ“Š Test Summary")
        print("=" * 70)
        print(f"  âœ… Passed: {self.passed}")
        print(f"  âŒ Failed: {self.failed}")
        print(f"  ğŸ“ˆ Success Rate: {self.passed / (self.passed + self.failed) * 100:.1f}%")
        print("=" * 70)
        
        return self.failed == 0


# =============================================================================
# Main
# =============================================================================

async def main():
    parser = argparse.ArgumentParser(description="Week 4 Day 1 Structured Analyzer Tests")
    parser.add_argument("--mock", action="store_true", default=True,
                       help="Use mock backend (default: True)")
    parser.add_argument("--live", action="store_true",
                       help="Use live Ollama backend")
    args = parser.parse_args()
    
    use_mock = not args.live
    
    runner = TestRunner(use_mock=use_mock)
    success = await runner.run_all_tests()
    
    sys.exit(0 if success else 1)


if __name__ == "__main__":
    asyncio.run(main())