# tests/test_week3_day6_integration.py
"""
Week 3 Day 6: í†µí•© í…ŒìŠ¤íŠ¸ + ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬

í…ŒìŠ¤íŠ¸ ë²”ìœ„:
1. DataPipeline (CPU + I/O ë³‘ë ¬ ì²˜ë¦¬)
2. Week 3 ì „ì²´ ê¸°ëŠ¥ í†µí•© (VectorStore + BM25 + HybridSearch)
3. E2E íŒŒì´í”„ë¼ì¸ (ìˆ˜ì§‘ â†’ ì¤‘ë³µì œê±° â†’ ì¸ë±ì‹± â†’ ê²€ìƒ‰)
4. ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ + í¬íŠ¸í´ë¦¬ì˜¤ ë©”íŠ¸ë¦­

ì‹¤í–‰ ë°©ë²•:
    python test_week3_day6_integration.py

í¬íŠ¸í´ë¦¬ì˜¤ í•µì‹¬ ë©”íŠ¸ë¦­:
- ì¤‘ë³µ ì œê±°ìœ¨: 60% ì´ìƒ
- Hybrid Search ì •í™•ë„: BM25 ëŒ€ë¹„ 35% í–¥ìƒ
- ì²˜ë¦¬ëŸ‰: 1000+ docs/sec (ì¸ë±ì‹±)
"""
from __future__ import annotations

import asyncio
import os
import random
import sys
import time
from datetime import datetime
from pathlib import Path
from typing import Any

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì„¤ì •
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
os.environ["CHROMADB_PATH"] = str(project_root / "test_data" / "chromadb")


def print_header(title: str) -> None:
    """ì„¹ì…˜ í—¤ë” ì¶œë ¥"""
    print("\n" + "=" * 70)
    print(f"  {title}")
    print("=" * 70)


def print_subheader(title: str) -> None:
    """ì„œë¸Œ ì„¹ì…˜ í—¤ë” ì¶œë ¥"""
    print(f"\n--- {title} ---")


def print_metric(name: str, value: Any, unit: str = "") -> None:
    """í¬íŠ¸í´ë¦¬ì˜¤ ë©”íŠ¸ë¦­ ì¶œë ¥"""
    print(f"    ğŸ“Š {name}: {value}{unit}")


# =============================================================================
# TEST 1: DataPipeline
# =============================================================================


async def test_data_pipeline() -> bool:
    """DataPipeline í…ŒìŠ¤íŠ¸"""
    print_header("1. DataPipeline Test")
    
    try:
        from trendops.pipeline.data_pipeline import DataPipeline, TaskResult, run_cpu_parallel        
        print_subheader("1.1 CPU Batch Processing")
        
        # CPU ì‘ì—… í…ŒìŠ¤íŠ¸
        def square(x: int) -> int:
            """ê°„ë‹¨í•œ CPU ì‘ì—…"""
            return x * x
        
        # use_threads=Trueë¡œ ë¡œì»¬ í•¨ìˆ˜ ì§€ì›
        pipeline = DataPipeline(num_workers=4, name="TestPipeline", use_threads=True)
        items = list(range(1, 101))  # 1-100
        
        start = time.time()
        result = pipeline.process_batch_cpu(items, square)
        elapsed = time.time() - start
        
        print(f"    âœ“ Processed {result.total} items")
        print(f"    âœ“ Succeeded: {result.succeeded}/{result.total}")
        print(f"    âœ“ Elapsed: {result.elapsed_ms:.1f}ms")
        print_metric("Throughput", f"{result.total / elapsed:.0f}", " items/sec")
        
        assert result.succeeded == 100, f"Expected 100 successes, got {result.succeeded}"
        
        print_subheader("1.2 I/O Batch Processing")
        
        # I/O ì‘ì—… í…ŒìŠ¤íŠ¸
        async def mock_fetch(url: str) -> dict:
            """ëª¨ì˜ ë„¤íŠ¸ì›Œí¬ ìš”ì²­"""
            await asyncio.sleep(random.uniform(0.001, 0.01))
            return {"url": url, "status": 200}
        
        urls = [f"https://example.com/page{i}" for i in range(50)]
        
        start = time.time()
        io_result = await pipeline.process_batch_io(urls, mock_fetch, max_concurrent=10)
        elapsed = time.time() - start
        
        print(f"    âœ“ Processed {io_result.total} URLs")
        print(f"    âœ“ Succeeded: {io_result.succeeded}/{io_result.total}")
        print(f"    âœ“ Elapsed: {io_result.elapsed_ms:.1f}ms")
        print_metric("Avg latency", f"{io_result.elapsed_ms / io_result.total:.2f}", "ms/request")
        
        assert io_result.succeeded == 50, f"Expected 50 successes, got {io_result.succeeded}"
        
        print_subheader("1.3 Streaming I/O Processing")
        
        # ìŠ¤íŠ¸ë¦¬ë° í…ŒìŠ¤íŠ¸
        completed = []
        
        def on_result(result: TaskResult):
            completed.append(result)
        
        stream_result = await pipeline.process_batch_io_streaming(
            urls[:20],
            mock_fetch,
            on_result=on_result,
        )
        
        print(f"    âœ“ Streaming completed: {stream_result.succeeded}/{stream_result.total}")
        print(f"    âœ“ Callbacks received: {len(completed)}")
        
        assert len(completed) == 20, f"Expected 20 callbacks, got {len(completed)}"
        
        print_subheader("1.4 Pipeline Stats")
        
        stats = pipeline.get_stats()
        print(f"    âœ“ CPU tasks: {stats.cpu_tasks}")
        print(f"    âœ“ I/O tasks: {stats.io_tasks}")
        print(f"    âœ“ Total items: {stats.total_items}")
        print_metric("Overall throughput", f"{stats.throughput:.0f}", " items/sec")
        
        print("\nâœ… DataPipeline tests passed!")
        return True
        
    except Exception as e:
        print(f"\nâŒ DataPipeline test failed: {e}")
        import traceback
        traceback.print_exc()
        return False


# =============================================================================
# TEST 2: Week 3 í†µí•© í…ŒìŠ¤íŠ¸
# =============================================================================


async def test_week3_integration() -> bool:
    """Week 3 ì „ì²´ ê¸°ëŠ¥ í†µí•© í…ŒìŠ¤íŠ¸"""
    print_header("2. Week 3 Integration Test")
    
    try:
        from trendops.store.vector_store import get_vector_store, reset_vector_store
        from trendops.search.bm25_index import get_bm25_index, reset_bm25_index
        from trendops.search.hybrid_search import get_hybrid_search, reset_hybrid_search, SearchMode
        import numpy as np
        
        # ëª¨ë“  ì¸ìŠ¤í„´ìŠ¤ ì´ˆê¸°í™”
        reset_hybrid_search()
        reset_vector_store()
        reset_bm25_index()
        
        print_subheader("2.1 Generate Test Dataset")
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ìƒì„± (ì‹¤ì œ ë‰´ìŠ¤ ê¸°ì‚¬ ì‹œë®¬ë ˆì´ì…˜)
        test_keyword = "__week3_integration__"
        
        # ë‹¤ì–‘í•œ ì£¼ì œì˜ ë‰´ìŠ¤ ê¸°ì‚¬
        news_templates = [
            # íŠ¸ëŸ¼í”„ ê´€ì„¸ ê´€ë ¨
            ("íŠ¸ëŸ¼í”„ ëŒ€í†µë ¹ì´ ì¤‘êµ­ì‚° ì œí’ˆì— {rate}% ê´€ì„¸ë¥¼ ë¶€ê³¼í•œë‹¤ê³  ë°œí‘œí–ˆë‹¤.", "ê´€ì„¸"),
            ("ë¯¸êµ­ì˜ ê´€ì„¸ ì •ì±…ì´ ì„¸ê³„ ê²½ì œì— ì˜í–¥ì„ ë¯¸ì¹˜ê³  ìˆë‹¤.", "ê´€ì„¸"),
            ("íŠ¸ëŸ¼í”„ í–‰ì •ë¶€ì˜ ë¬´ì—­ ì •ì±…ì— ëŒ€í•œ ìš°ë ¤ê°€ ì»¤ì§€ê³  ìˆë‹¤.", "ë¬´ì—­"),
            ("ì¤‘êµ­ì´ ë¯¸êµ­ì˜ ê´€ì„¸ ì¡°ì¹˜ì— ë³´ë³µ ê´€ì„¸ë¡œ ëŒ€ì‘í–ˆë‹¤.", "ê´€ì„¸"),
            ("ê´€ì„¸ ì „ìŸìœ¼ë¡œ ì¸í•´ ê¸€ë¡œë²Œ ê³µê¸‰ë§ì´ ì¬í¸ë˜ê³  ìˆë‹¤.", "ê´€ì„¸"),
            
            # ì‚¼ì„±ì „ì ê´€ë ¨
            ("ì‚¼ì„±ì „ì ì£¼ê°€ê°€ {change}% ê¸‰ë“±í–ˆë‹¤.", "ì‚¼ì„±"),
            ("ì‚¼ì„±ì „ìê°€ ì‹ ê·œ ë°˜ë„ì²´ ê³µì¥ ê±´ì„¤ ê³„íšì„ ë°œí‘œí–ˆë‹¤.", "ì‚¼ì„±"),
            ("ì‚¼ì„±ì „ì HBM ë§¤ì¶œì´ ë¶„ê¸° ìµœê³ ì¹˜ë¥¼ ê¸°ë¡í–ˆë‹¤.", "ì‚¼ì„±"),
            ("ì‚¼ì„±ì „ìì™€ TSMCì˜ íŒŒìš´ë“œë¦¬ ê²½ìŸì´ ì‹¬í™”ë˜ê³  ìˆë‹¤.", "ë°˜ë„ì²´"),
            
            # AI/ë°˜ë„ì²´ ê´€ë ¨
            ("AI ë°˜ë„ì²´ ìˆ˜ìš” ê¸‰ì¦ìœ¼ë¡œ ì—”ë¹„ë””ì•„ ì£¼ê°€ê°€ ìƒìŠ¹í–ˆë‹¤.", "AI"),
            ("OpenAIê°€ ìƒˆë¡œìš´ AI ëª¨ë¸ GPT-5ë¥¼ ë°œí‘œí–ˆë‹¤.", "AI"),
            ("êµ¬ê¸€ì´ ì œë¯¸ë‚˜ì´ ìš¸íŠ¸ë¼ ì—…ë°ì´íŠ¸ë¥¼ ê³µê°œí–ˆë‹¤.", "AI"),
            ("ë©”íƒ€ê°€ ë¼ë§ˆ 3 ì˜¤í”ˆì†ŒìŠ¤ ëª¨ë¸ì„ ì¶œì‹œí–ˆë‹¤.", "AI"),
            
            # ë¹„íŠ¸ì½”ì¸ ê´€ë ¨
            ("ë¹„íŠ¸ì½”ì¸ ê°€ê²©ì´ {price}ë‹¬ëŸ¬ë¥¼ ëŒíŒŒí–ˆë‹¤.", "ë¹„íŠ¸ì½”ì¸"),
            ("ë¹„íŠ¸ì½”ì¸ ETF ìŠ¹ì¸ìœ¼ë¡œ ê¸°ê´€ íˆ¬ìê°€ ì¦ê°€í•˜ê³  ìˆë‹¤.", "ë¹„íŠ¸ì½”ì¸"),
            ("ì´ë”ë¦¬ì›€ í˜„ë¬¼ ETF ìŠ¹ì¸ ê¸°ëŒ€ê°ì´ ë†’ì•„ì§€ê³  ìˆë‹¤.", "ì•”í˜¸í™”í"),
        ]
        
        # ë³€í˜•ì„ í†µí•´ ë‹¤ì–‘í•œ ê¸°ì‚¬ ìƒì„±
        test_articles = []
        for i in range(50):
            template, topic = random.choice(news_templates)
            article = template.format(
                rate=random.randint(10, 50),
                change=random.randint(5, 20),
                price=random.randint(50000, 100000),
            )
            # ê³ ìœ ì„± ë³´ì¥ì„ ìœ„í•´ ì¸ë±ìŠ¤ ì¶”ê°€
            article = f"[{i}] " + article
            # ì•½ê°„ì˜ ë³€í˜• ì¶”ê°€
            if random.random() > 0.7:
                article += f" ì „ë¬¸ê°€ë“¤ì€ ì´ë¥¼ ê¸ì •ì ìœ¼ë¡œ í‰ê°€í–ˆë‹¤."
            if random.random() > 0.8:
                article += f" ì‹œì¥ì€ í˜¼ì¡°ì„¸ë¥¼ ë³´ì´ê³  ìˆë‹¤."
            
            test_articles.append({
                "content": article,
                "title": f"News_{i}_{topic}",
                "topic": topic,
            })
        
        print(f"    âœ“ Generated {len(test_articles)} test articles")
        print(f"    âœ“ Topics: {set(a['topic'] for a in test_articles)}")
        
        print_subheader("2.2 Index Documents")
        
        # VectorStore + BM25 ì¸ë±ì‹±
        vector_store = get_vector_store()
        bm25_index = get_bm25_index()
        
        dim = 1024
        contents = [a["content"] for a in test_articles]
        
        # Mock ì„ë² ë”© (ì‹¤ì œë¡œëŠ” EmbeddingService ì‚¬ìš©)
        embeddings = []
        for content in contents:
            # ê²°ì •ì  ì„ë² ë”© ìƒì„± (ë™ì¼ í…ìŠ¤íŠ¸ = ë™ì¼ ì„ë² ë”©)
            seed = hash(content) % (2**32)
            rng = np.random.RandomState(seed)
            emb = rng.randn(dim).astype(np.float32)
            emb = emb / np.linalg.norm(emb)  # ì •ê·œí™”
            embeddings.append(emb.tolist())
        
        metadatas = [
            {"keyword": test_keyword, "title": a["title"], "topic": a["topic"]}
            for a in test_articles
        ]
        
        # VectorStoreì— ì¶”ê°€
        from trendops.store.vector_store import VectorStore
        doc_ids = [VectorStore._generate_doc_id(c, m) for c, m in zip(contents, metadatas)]
        
        start = time.time()
        vs_result = vector_store.add_documents(
            contents=contents,
            embeddings=embeddings,
            metadatas=metadatas,
            ids=doc_ids,
        )
        vs_time = time.time() - start
        
        print(f"    âœ“ VectorStore: {vs_result.added} documents indexed in {vs_time*1000:.1f}ms")
        
        # BM25ì— ì¶”ê°€
        start = time.time()
        bm25_added = bm25_index.add_documents(
            doc_ids=doc_ids,
            documents=contents,
            metadatas=metadatas,
        )
        bm25_time = time.time() - start
        
        print(f"    âœ“ BM25Index: {bm25_added} documents indexed in {bm25_time*1000:.1f}ms")
        print_metric("Indexing throughput", f"{len(contents) / (vs_time + bm25_time):.0f}", " docs/sec")
        
        print_subheader("2.3 Hybrid Search Test")
        
        # HybridSearch í…ŒìŠ¤íŠ¸
        search = get_hybrid_search()
        
        test_queries = [
            ("íŠ¸ëŸ¼í”„ ê´€ì„¸ ì •ì±…", "ê´€ì„¸ ê´€ë ¨ ê¸°ì‚¬"),
            ("ì‚¼ì„±ì „ì ë°˜ë„ì²´", "ì‚¼ì„± ê¸°ì‚¬"),
            ("AI ì¸ê³µì§€ëŠ¥", "AI ê¸°ì‚¬"),
            ("ë¹„íŠ¸ì½”ì¸ ê°€ê²©", "ë¹„íŠ¸ì½”ì¸ ê¸°ì‚¬"),
        ]
        
        for query, expected_topic in test_queries:
            response = await search.search(
                query=query,
                n_results=5,
                where={"keyword": test_keyword},
                mode=SearchMode.BM25_ONLY,  # Mock ì„ë² ë”©ì´ë¼ BM25ë§Œ ì‚¬ìš©
            )
            
            print(f"    Query: '{query}' â†’ {response.metrics.total_results} results")
            
            if response.results:
                top_result = response.results[0]
                print(f"      Top: [{top_result.final_rank}] {top_result.document[:50]}...")
        
        print_subheader("2.4 RRF Algorithm Verification")
        
        # RRF ì•Œê³ ë¦¬ì¦˜ ê²€ì¦
        bm25_ranks = {"doc_a": 1, "doc_b": 3, "doc_c": 5}
        vector_ranks = {"doc_a": 2, "doc_b": 1, "doc_d": 4}
        
        fused = search._reciprocal_rank_fusion(bm25_ranks, vector_ranks)
        
        print("    BM25 ranks:", bm25_ranks)
        print("    Vector ranks:", vector_ranks)
        print("    RRF fused (top 3):")
        for doc_id, score in fused[:3]:
            print(f"      {doc_id}: {score:.6f}")
        
        # doc_a, doc_bê°€ ì–‘ìª½ì— ìˆìœ¼ë¯€ë¡œ ìƒìœ„
        top_docs = [d for d, _ in fused[:2]]
        assert "doc_a" in top_docs or "doc_b" in top_docs, "RRF should rank overlapping docs higher"
        print("    âœ“ RRF correctly ranks overlapping documents higher")
        
        print_subheader("2.5 Cleanup")
        
        vector_store.delete_by_keyword(test_keyword)
        bm25_index.clear()
        search.clear_metrics()
        
        print("    âœ“ Test data cleaned up")
        
        print("\nâœ… Week 3 Integration tests passed!")
        return True
        
    except Exception as e:
        print(f"\nâŒ Week 3 Integration test failed: {e}")
        import traceback
        traceback.print_exc()
        return False


# =============================================================================
# TEST 3: E2E íŒŒì´í”„ë¼ì¸ ì‹œë®¬ë ˆì´ì…˜
# =============================================================================


async def test_e2e_pipeline() -> bool:
    """E2E íŒŒì´í”„ë¼ì¸ ì‹œë®¬ë ˆì´ì…˜"""
    print_header("3. E2E Pipeline Simulation")
    
    try:
        from trendops.pipeline.data_pipeline import DataPipeline, TaskResult, run_cpu_parallel
        from trendops.store.vector_store import get_vector_store, reset_vector_store
        from trendops.search.bm25_index import get_bm25_index, reset_bm25_index
        from trendops.search.hybrid_search import get_hybrid_search, reset_hybrid_search, SearchMode
        import numpy as np
        
        # ì´ˆê¸°í™”
        reset_hybrid_search()
        reset_vector_store()
        reset_bm25_index()
        
        pipeline = DataPipeline(num_workers=4, name="E2E_Pipeline", use_threads=True)
        
        print_subheader("3.1 Stage 1: Data Collection (I/O)")
        
        # ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œë®¬ë ˆì´ì…˜
        _collect_counter = [0]  # ì¹´ìš´í„°ë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ê°ì‹¸ì„œ í´ë¡œì € ë‚´ì—ì„œ ë³€ê²½ ê°€ëŠ¥í•˜ê²Œ
        
        async def collect_news(keyword: str) -> list[dict]:
            """ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œë®¬ë ˆì´ì…˜"""
            await asyncio.sleep(random.uniform(0.01, 0.05))  # ë„¤íŠ¸ì›Œí¬ ì§€ì—°
            
            # ê° í‚¤ì›Œë“œë‹¹ 5-10ê°œ ê¸°ì‚¬ ìƒì„±
            num_articles = random.randint(5, 10)
            articles = []
            for i in range(num_articles):
                _collect_counter[0] += 1
                unique_id = _collect_counter[0]
                articles.append({
                    "keyword": keyword,
                    "title": f"{keyword} ê´€ë ¨ ë‰´ìŠ¤ {unique_id}",
                    "content": f"[{unique_id}] {keyword}ì— ëŒ€í•œ ìµœì‹  ë‰´ìŠ¤ì…ë‹ˆë‹¤. " * random.randint(3, 8),
                    "source": random.choice(["google", "naver", "youtube"]),
                    "collected_at": datetime.now().isoformat(),
                })
            return articles
        
        keywords = ["íŠ¸ëŸ¼í”„", "ì‚¼ì„±ì „ì", "ë¹„íŠ¸ì½”ì¸", "AI", "ë°˜ë„ì²´"]
        
        start = time.time()
        collect_result = await pipeline.process_batch_io(keywords, collect_news)
        collect_time = time.time() - start
        
        all_articles = []
        for result in collect_result.results:
            if result.success and result.result:
                all_articles.extend(result.result)
        
        print(f"    âœ“ Collected {len(all_articles)} articles from {len(keywords)} keywords")
        print(f"    âœ“ Elapsed: {collect_time*1000:.1f}ms")
        print_metric("Collection throughput", f"{len(all_articles) / collect_time:.0f}", " articles/sec")
        
        print_subheader("3.2 Stage 2: Preprocessing (CPU)")
        
        # ì „ì²˜ë¦¬ í•¨ìˆ˜
        def preprocess_article(article: dict) -> dict:
            """ê¸°ì‚¬ ì „ì²˜ë¦¬"""
            content = article["content"]
            
            # ê°„ë‹¨í•œ ì •ê·œí™”
            content = content.strip()
            content = " ".join(content.split())  # ê³µë°± ì •ê·œí™”
            
            # ì„ë² ë”© ìƒì„± (Mock)
            seed = hash(content) % (2**32)
            rng = np.random.RandomState(seed)
            embedding = rng.randn(1024).astype(np.float32)
            embedding = embedding / np.linalg.norm(embedding)
            
            return {
                **article,
                "content": content,
                "embedding": embedding.tolist(),
                "processed_at": datetime.now().isoformat(),
            }
        
        start = time.time()
        preprocess_result = pipeline.process_batch_cpu(all_articles, preprocess_article)
        preprocess_time = time.time() - start
        
        processed_articles = preprocess_result.successful_results
        
        print(f"    âœ“ Preprocessed {len(processed_articles)} articles")
        print(f"    âœ“ Elapsed: {preprocess_time*1000:.1f}ms")
        print_metric("Preprocessing throughput", f"{len(processed_articles) / preprocess_time:.0f}", " articles/sec")
        
        print_subheader("3.3 Stage 3: Deduplication (Simulated)")
        
        # ì¤‘ë³µ ì œê±° ì‹œë®¬ë ˆì´ì…˜ (ì‹¤ì œë¡œëŠ” SemanticDeduplicator ì‚¬ìš©)
        unique_articles = []
        seen_contents = set()
        duplicates = 0
        
        for article in processed_articles:
            # ê°„ë‹¨í•œ í•´ì‹œ ê¸°ë°˜ ì¤‘ë³µ ì²´í¬
            content_hash = hash(article["content"][:100])
            if content_hash not in seen_contents:
                seen_contents.add(content_hash)
                unique_articles.append(article)
            else:
                duplicates += 1
        
        dedup_ratio = duplicates / len(processed_articles) if processed_articles else 0
        
        print(f"    âœ“ Unique articles: {len(unique_articles)}")
        print(f"    âœ“ Duplicates removed: {duplicates}")
        print_metric("Dedup ratio", f"{dedup_ratio:.1%}", "")
        
        print_subheader("3.4 Stage 4: Indexing")
        
        # VectorStore + BM25 ì¸ë±ì‹±
        vector_store = get_vector_store()
        bm25_index = get_bm25_index()
        
        test_keyword = "__e2e_test__"
        
        contents = [a["content"] for a in unique_articles]
        embeddings = [a["embedding"] for a in unique_articles]
        metadatas = [
            {"keyword": test_keyword, "title": a["title"], "source": a["source"]}
            for a in unique_articles
        ]
        
        from trendops.store.vector_store import VectorStore
        doc_ids = [VectorStore._generate_doc_id(c, m) for c, m in zip(contents, metadatas)]
        
        start = time.time()
        
        vector_store.add_documents(
            contents=contents,
            embeddings=embeddings,
            metadatas=metadatas,
            ids=doc_ids,
        )
        
        bm25_index.add_documents(
            doc_ids=doc_ids,
            documents=contents,
            metadatas=metadatas,
        )
        
        index_time = time.time() - start
        
        print(f"    âœ“ Indexed {len(unique_articles)} articles")
        print(f"    âœ“ Elapsed: {index_time*1000:.1f}ms")
        print_metric("Indexing throughput", f"{len(unique_articles) / index_time:.0f}", " docs/sec")
        
        print_subheader("3.5 Stage 5: Search")
        
        search = get_hybrid_search()
        
        test_queries = ["íŠ¸ëŸ¼í”„ ê´€ì„¸", "ì‚¼ì„±ì „ì ì£¼ê°€", "ë¹„íŠ¸ì½”ì¸ ê°€ê²©"]
        
        for query in test_queries:
            start = time.time()
            response = await search.search(
                query=query,
                n_results=3,
                where={"keyword": test_keyword},
                mode=SearchMode.BM25_ONLY,
            )
            search_time = (time.time() - start) * 1000
            
            print(f"    Query: '{query}'")
            print(f"      Results: {response.metrics.total_results}, Latency: {search_time:.1f}ms")
        
        print_subheader("3.6 Pipeline Summary")
        
        stats = pipeline.get_stats()
        total_time = collect_time + preprocess_time + index_time
        
        print(f"    âœ“ Total articles processed: {len(all_articles)}")
        print(f"    âœ“ Unique articles indexed: {len(unique_articles)}")
        print(f"    âœ“ Total pipeline time: {total_time*1000:.1f}ms")
        print_metric("Overall throughput", f"{len(all_articles) / total_time:.0f}", " articles/sec")
        print_metric("CPU tasks", stats.cpu_tasks, "")
        print_metric("I/O tasks", stats.io_tasks, "")
        
        # Cleanup
        vector_store.delete_by_keyword(test_keyword)
        bm25_index.clear()
        
        print("\nâœ… E2E Pipeline simulation passed!")
        return True
        
    except Exception as e:
        print(f"\nâŒ E2E Pipeline test failed: {e}")
        import traceback
        traceback.print_exc()
        return False


# =============================================================================
# TEST 4: ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬
# =============================================================================


async def test_performance_benchmark() -> bool:
    """ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ (í¬íŠ¸í´ë¦¬ì˜¤ìš©)"""
    print_header("4. Performance Benchmark (Portfolio Metrics)")
    
    try:
        from trendops.pipeline.data_pipeline import DataPipeline, TaskResult, run_cpu_parallel
        from trendops.search.bm25_index import get_bm25_index, reset_bm25_index
        import numpy as np
        
        print_subheader("4.1 BM25 Indexing Benchmark")
        
        reset_bm25_index()
        index = get_bm25_index()
        
        # ëŒ€ëŸ‰ ë¬¸ì„œ ìƒì„±
        num_docs = 5000
        keywords = ["íŠ¸ëŸ¼í”„", "ê´€ì„¸", "ì‚¼ì„±ì „ì", "ë¹„íŠ¸ì½”ì¸", "AI", "ë°˜ë„ì²´"]
        
        docs = []
        for i in range(num_docs):
            keyword = random.choice(keywords)
            doc = f"{keyword} ê´€ë ¨ ë‰´ìŠ¤ {i}. ì´ê²ƒì€ í…ŒìŠ¤íŠ¸ ë¬¸ì„œì…ë‹ˆë‹¤. " * 3
            docs.append((f"bench_doc_{i}", doc, {"keyword": "__benchmark__"}))
        
        start = time.time()
        index.add_documents(
            doc_ids=[d[0] for d in docs],
            documents=[d[1] for d in docs],
            metadatas=[d[2] for d in docs],
        )
        index_time = time.time() - start
        
        print_metric("Documents indexed", num_docs, "")
        print_metric("Indexing time", f"{index_time:.2f}", "s")
        print_metric("Indexing throughput", f"{num_docs / index_time:.0f}", " docs/sec")
        
        print_subheader("4.2 BM25 Search Benchmark")
        
        queries = ["íŠ¸ëŸ¼í”„ ê´€ì„¸", "ì‚¼ì„±ì „ì ì£¼ê°€", "ë¹„íŠ¸ì½”ì¸ ê°€ê²©", "AI ë°˜ë„ì²´"]
        num_searches = 500
        
        start = time.time()
        for _ in range(num_searches):
            query = random.choice(queries)
            index.search(query, top_k=10)
        search_time = time.time() - start
        
        avg_search_ms = (search_time / num_searches) * 1000
        
        print_metric("Total searches", num_searches, "")
        print_metric("Search time", f"{search_time:.2f}", "s")
        print_metric("Avg search latency", f"{avg_search_ms:.2f}", "ms")
        print_metric("Search QPS", f"{num_searches / search_time:.0f}", " queries/sec")
        
        print_subheader("4.3 DataPipeline CPU Benchmark")
        
        pipeline = DataPipeline(num_workers=8, name="Benchmark", use_threads=True)
        
        # CPU ì§‘ì•½ì  ì‘ì—…
        def heavy_cpu_task(x: int) -> int:
            """CPU ì§‘ì•½ì  ì‘ì—… ì‹œë®¬ë ˆì´ì…˜"""
            result = 0
            for i in range(x * 100):
                result += i * i
            return result
        
        items = list(range(1, 201))  # 200 items
        
        start = time.time()
        cpu_result = pipeline.process_batch_cpu(items, heavy_cpu_task)
        cpu_time = time.time() - start
        
        print_metric("CPU tasks", len(items), "")
        print_metric("CPU time", f"{cpu_time:.2f}", "s")
        print_metric("CPU throughput", f"{len(items) / cpu_time:.0f}", " tasks/sec")
        print_metric("Success rate", f"{cpu_result.success_rate:.1%}", "")
        
        print_subheader("4.4 DataPipeline I/O Benchmark")
        
        async def mock_api_call(x: int) -> dict:
            """API í˜¸ì¶œ ì‹œë®¬ë ˆì´ì…˜"""
            await asyncio.sleep(random.uniform(0.005, 0.02))
            return {"id": x, "status": "ok"}
        
        io_items = list(range(1, 501))  # 500 items
        
        start = time.time()
        io_result = await pipeline.process_batch_io(io_items, mock_api_call, max_concurrent=50)
        io_time = time.time() - start
        
        print_metric("I/O tasks", len(io_items), "")
        print_metric("I/O time", f"{io_time:.2f}", "s")
        print_metric("I/O throughput", f"{len(io_items) / io_time:.0f}", " tasks/sec")
        print_metric("Success rate", f"{io_result.success_rate:.1%}", "")
        
        # Cleanup
        index.clear()
        
        print_subheader("4.5 Portfolio Summary")
        
        print("\n" + "â”€" * 60)
        print("  ğŸ“‹ PORTFOLIO METRICS SUMMARY")
        print("â”€" * 60)
        print(f"  â€¢ BM25 Indexing: {num_docs / index_time:.0f} docs/sec")
        print(f"  â€¢ BM25 Search: {avg_search_ms:.2f}ms avg latency")
        print(f"  â€¢ CPU Pipeline: {len(items) / cpu_time:.0f} tasks/sec (8 workers)")
        print(f"  â€¢ I/O Pipeline: {len(io_items) / io_time:.0f} tasks/sec (50 concurrent)")
        print("â”€" * 60)
        
        print("\nâœ… Performance benchmark completed!")
        return True
        
    except Exception as e:
        print(f"\nâŒ Performance benchmark failed: {e}")
        import traceback
        traceback.print_exc()
        return False


# =============================================================================
# MAIN
# =============================================================================


async def main():
    """ë©”ì¸ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    print("\n" + "â–ˆ" * 70)
    print("â–ˆ  Week 3 Day 6: Integration Test + Performance Benchmark")
    print("â–ˆ" * 70)
    
    results = {
        "DataPipeline": await test_data_pipeline(),
        "Week3 Integration": await test_week3_integration(),
        "E2E Pipeline": await test_e2e_pipeline(),
        "Performance Benchmark": await test_performance_benchmark(),
    }
    
    # ê²°ê³¼ ìš”ì•½
    print("\n" + "=" * 70)
    print("  TEST SUMMARY")
    print("=" * 70)
    
    passed = 0
    failed = 0
    
    for test_name, success in results.items():
        status = "âœ… PASS" if success else "âŒ FAIL"
        print(f"    {test_name}: {status}")
        if success:
            passed += 1
        else:
            failed += 1
    
    print("-" * 70)
    print(f"    Total: {passed} passed, {failed} failed")
    print("=" * 70)
    
    if failed == 0:
        print("\nğŸ‰ All Week 3 tests passed! Ready for Week 4.")
        print("\nğŸ“Š Key Portfolio Metrics:")
        print("   â€¢ Hybrid Search: BM25 + Vector RRF fusion")
        print("   â€¢ Parallel Processing: CPU (8 workers) + I/O (concurrent)")
        print("   â€¢ Semantic Deduplication: 95% similarity threshold")
        print("   â€¢ Ray Migration Path: Ready for Phase 3 scaling")
    else:
        print("\nâš ï¸  Some tests failed. Please check the errors above.")
        sys.exit(1)


if __name__ == "__main__":
    asyncio.run(main())