# scripts/real_e2e_pipeline.py
"""
TrendOps Real End-to-End Pipeline (RAG Enabled)

Upgrade Week 5:
- Hybrid Search ê¸°ë°˜ RAG (ê²€ìƒ‰ ì¦ê°• ìƒì„±) ì ìš©
- ì „ì²´ í‚¤ì›Œë“œ(Top 10) ìˆ˜ì§‘ ë° ë¶„ì„ ìë™í™”
- ì¼ì¼ ë¦¬í¬íŠ¸ ë°ì´í„° ìë™ ì €ì¥
"""
import asyncio
import json
import os
import sys
import time
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì„¤ì •
project_root = Path(__file__).parent.parent
sys.path.append(str(project_root))

from trendops.collector.collector_rss import RSSCollector
from trendops.collector.collector_youtube import YouTubeCollector

# ê²€ìƒ‰ ë° ì¸ë±ì‹± ëª¨ë“ˆ
from trendops.search.bm25_index import get_bm25_index
from trendops.search.hybrid_search import SearchMode, get_hybrid_search
from trendops.service.deduplicator import get_deduplicator
from trendops.trigger.trigger_google import GoogleTrendTrigger
from trendops.utils.logger import get_logger

# [ì¶”ê°€ë¨] ë¦¬í¬íŠ¸ ì„œë¹„ìŠ¤ (ì¼ì¼ ë¦¬í¬íŠ¸ ìƒì„±ì„ ìœ„í•œ ë°ì´í„° ì €ì¥)
try:
    from trendops.publisher.report_service import ReportService

    report_service = ReportService()
except ImportError:
    report_service = None

# Rich ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì • (Table ì¶”ê°€)
try:
    from rich import print as rprint
    from rich.console import Console
    from rich.panel import Panel
    from rich.table import Table
    from rich.tree import Tree

    console = Console()
except ImportError:
    console = None

logger = get_logger("pipeline_e2e")


def print_stage(title: str):
    if console:
        console.print(Panel(f"[bold white]{title}[/bold white]", style="bold blue"))
    else:
        print(f"\nâ”â”â” {title} â”â”â”")


def print_success(msg: str):
    if console:
        console.print(f"[bold green]âœ“ {msg}[/bold green]")
    else:
        print(f"âœ“ {msg}")


def print_error(msg: str):
    if console:
        console.print(f"[bold red]âœ— {msg}[/bold red]")
    else:
        print(f"âœ— {msg}")


# =============================================================================
# Stage 1: Trigger
# =============================================================================
async def stage_trigger(max_keywords: int = 10):
    print_stage("Stage 1: TRIGGER - íŠ¸ë Œë“œ ê°ì§€")
    try:
        trigger = GoogleTrendTrigger()
        trend_keywords = await trigger.fetch_trends()

        # ì „ì²´ í‚¤ì›Œë“œ ê°€ì ¸ì˜¤ê¸°
        keywords = [
            {"keyword": tk.keyword, "score": tk.trend_score, "source": tk.source}
            for tk in trend_keywords[:max_keywords]
        ]

        print_success(f"Google Trendsì—ì„œ {len(keywords)}ê°œ í‚¤ì›Œë“œ ê°ì§€")

        # ê²°ê³¼ í…Œì´ë¸” ì¶œë ¥
        if console and keywords:
            table = Table(show_header=True, header_style="bold cyan")
            table.add_column("Rank", width=6)
            table.add_column("Keyword", width=25)
            table.add_column("Score", width=10)
            table.add_column("Source", width=15)

            for i, kw in enumerate(keywords, 1):
                source_str = kw.get("source", "unknown")
                if hasattr(source_str, "value"):
                    source_str = source_str.value
                table.add_row(f"#{i}", kw["keyword"], f"{kw['score']:.1f}", str(source_str))

            console.print(table)

        return keywords
    except Exception as e:
        print_error(f"íŠ¸ë¦¬ê±° ì‹¤íŒ¨: {e}")
        return []


# =============================================================================
# Stage 2: Collector (Hybrid)
# =============================================================================
async def stage_collection(keywords: list[dict], max_articles: int = 15):
    print_stage("Stage 2: COLLECTOR - ë‰´ìŠ¤/ìœ íŠœë¸Œ ìˆ˜ì§‘")
    start = time.time()
    all_articles = []

    # [ìˆ˜ì • 1] ìƒìœ„ 3ê°œ ì œí•œ ì œê±° -> ì „ì²´ í‚¤ì›Œë“œ ëŒ€ìƒ ìˆ˜ì§‘
    target_keywords = [kw["keyword"] for kw in keywords]

    # 1. RSS ë‰´ìŠ¤ ìˆ˜ì§‘ (ì „ì²´)
    try:
        print(f"ğŸ“¡ RSS ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘... ({len(target_keywords)}ê°œ í‚¤ì›Œë“œ)")
        async with RSSCollector(max_results=max_articles) as rss:
            for kw in target_keywords:
                docs = await rss.fetch(kw)
                for doc in docs:
                    all_articles.append(doc_to_dict(doc))
                await asyncio.sleep(0.1)
    except Exception as e:
        print_error(f"RSS ìˆ˜ì§‘ ì—ëŸ¬: {e}")

    # 2. YouTube ëŒ“ê¸€ ìˆ˜ì§‘ (ì‹œê°„ ê´€ê³„ìƒ ìƒìœ„ 3ê°œë§Œ)
    try:
        yt_targets = target_keywords[:3]  # ìœ íŠœë¸ŒëŠ” ìƒìœ„ 3ê°œë§Œ
        if yt_targets:
            print(f"ğŸ¬ YouTube ì—¬ë¡  ìˆ˜ì§‘ ì‹œì‘... (ìƒìœ„ {len(yt_targets)}ê°œ)")
            async with YouTubeCollector(headless=True) as yt:
                for kw in yt_targets:
                    # í‚¤ì›Œë“œë³„ë¡œ ìˆœì°¨ ìˆ˜ì§‘
                    yt_docs = await yt.fetch(keyword=kw, max_videos=2, comments_per_video=5)
                    if yt_docs:
                        for doc in yt_docs:
                            all_articles.append(doc_to_dict(doc))
                        print(f"   - '{kw}': ëŒ“ê¸€ {len(yt_docs)}ê°œ")
    except Exception as e:
        print_error(f"YouTube ìˆ˜ì§‘ ì—ëŸ¬: {e}")

    if console and all_articles:
        tree = Tree(f"ğŸ“¦ ìˆ˜ì§‘ ê²°ê³¼ (ì´ {len(all_articles)}ê±´)")
        sources = {}
        for a in all_articles:
            src = a.get("source", "unknown")
            sources[src] = sources.get(src, 0) + 1
        for src, cnt in sources.items():
            tree.add(f"[yellow]{src}[/]: {cnt}ê±´")
        console.print(tree)

    return all_articles


def doc_to_dict(doc):
    return {
        "title": doc.title,
        "link": doc.link,
        "summary": doc.summary,
        "keyword": doc.keyword,
        "source": doc.source,
        "published": str(doc.published),
        "metadata": doc.metadata,
    }


# =============================================================================
# Stage 3: Deduplication & Indexing
# =============================================================================
async def stage_deduplication_and_indexing(articles: list[dict]):
    print_stage("Stage 3: DEDUPLICATION & INDEXING")
    if not articles:
        return []

    try:
        deduplicator = get_deduplicator()
        dedup_items = []

        for art in articles:
            text = f"{art.get('title','')} {art.get('summary','')}"
            meta = {
                "keyword": art.get("keyword", "unknown"),
                "title": art.get("title", "")[:100],
                "source": str(art.get("source", "unknown")),
                "link": art.get("link", ""),
                "published": str(art.get("published", "")),
            }
            if "metadata" in art and isinstance(art["metadata"], dict):
                for k, v in art["metadata"].items():
                    meta[k] = str(v)

            dedup_items.append((text, meta))

        results = await deduplicator.add_batch_unique(items=dedup_items)

        unique_articles = []
        new_doc_ids = []
        new_docs_content = []
        new_docs_meta = []

        for art, res, item in zip(articles, results, dedup_items):
            if res.is_added:
                unique_articles.append(art)
                if res.doc_id:
                    new_doc_ids.append(res.doc_id)
                    new_docs_content.append(item[0])
                    new_docs_meta.append(item[1])

        print_success(
            f"ì¤‘ë³µ ì œê±°: {len(articles)}ê±´ â†’ {len(unique_articles)}ê±´ (ì‹ ê·œ {len(unique_articles)}ê±´)"
        )

        if new_doc_ids:
            bm25 = get_bm25_index()
            added_count = bm25.add_documents(
                doc_ids=new_doc_ids, documents=new_docs_content, metadatas=new_docs_meta
            )
            print_success(f"BM25 ì¸ë±ì‹±: {added_count}ê±´ ì¶”ê°€ ì™„ë£Œ")

        return unique_articles

    except Exception as e:
        print_error(f"ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        return articles


# =============================================================================
# Stage 3.5: RAG Search
# =============================================================================
async def stage_rag_search(keyword: str, current_articles: list[dict]):
    try:
        search_engine = get_hybrid_search()
        response = await search_engine.search(query=keyword, n_results=5, mode=SearchMode.HYBRID)

        context_docs = []
        if response.results:
            for res in response.results:
                context_docs.append(
                    {
                        "content": res.document[:200] + "...",
                        "source": res.metadata.get("source", "unknown"),
                        "date": res.metadata.get("published", "unknown"),
                    }
                )
        return context_docs
    except Exception:
        return []


# =============================================================================
# Stage 4: LLM Analysis (Self-Correction & Guardrail)
# =============================================================================
async def stage_llm_analysis(
    keyword: str,
    articles: list[dict],
    ollama_url: str = "http://localhost:11434",
    model: str = "exaone3.5",
    max_retries: int = 3,
) -> tuple[dict | None, dict]:
    start = time.time()

    # 1. ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
    context_parts = []
    for i, article in enumerate(articles[:10], 1):
        pub_date = article.get("published", "")[:10]
        context_parts.append(
            f"[{i}] {article['title']} ({pub_date})\n   - {article['summary'][:200]}"
        )
    context = "\n".join(context_parts)

    # 2. ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
    system_prompt = """ë‹¹ì‹ ì€ ëƒ‰ì² í•œ íŠ¸ë Œë“œ ë¶„ì„ AIì…ë‹ˆë‹¤.
ê°ê´€ì ì¸ ì‚¬ì‹¤ì— ê¸°ë°˜í•˜ì—¬ ë¶„ì„í•˜ê³ , ê°ì •ì ì´ê±°ë‚˜ í¸í–¥ëœ í‘œí˜„ì„ ë°°ì œí•˜ì„¸ìš”.

[ì¤‘ìš”]
1. ì…ë ¥ëœ ë‰´ìŠ¤ê°€ ì˜ì–´ë¼ë„, ë¶„ì„ ê²°ê³¼ëŠ” ë°˜ë“œì‹œ **í•œêµ­ì–´(Korean)**ë¡œ ì‘ì„±í•´ì•¼ í•©ë‹ˆë‹¤.
2. ì „ë¬¸ ìš©ì–´ëŠ” ê´„í˜¸ ì•ˆì— ì˜ë¬¸ì„ ë³‘ê¸°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ë°˜ë“œì‹œ ì•„ë˜ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”:
{
  "main_cause": "í•µì‹¬ ì›ì¸ (1ë¬¸ì¥, í•œêµ­ì–´)",
  "sentiment": {"positive": 0.0~1.0, "negative": 0.0~1.0, "neutral": 0.0~1.0},
  "key_opinions": ["í•µì‹¬ ì—¬ë¡  1 (í•œêµ­ì–´)", "í•µì‹¬ ì—¬ë¡  2 (í•œêµ­ì–´)", "í•µì‹¬ ì—¬ë¡  3 (í•œêµ­ì–´)"],
  "summary": "ì „ì²´ ìš”ì•½ (3ë¬¸ì¥ ì´ìƒ, í•œêµ­ì–´)"
}
"""
    base_user_prompt = (
        f"í‚¤ì›Œë“œ: '{keyword}'\n\n[ê´€ë ¨ ë¬¸ì„œ]\n{context}\n\nìœ„ ë¬¸ì„œë¥¼ ë¶„ì„í•˜ì—¬ JSONìœ¼ë¡œ ì¶œë ¥í•˜ì„¸ìš”."
    )

    try:
        from trendops.analyst.guardrail import ContentGuardrail, GuardrailAction

        guardrail = ContentGuardrail(use_mock=False)
    except ImportError:
        guardrail = None

    import aiohttp

    current_prompt = base_user_prompt
    final_analysis = None

    async with aiohttp.ClientSession() as session:
        for attempt in range(1, max_retries + 1):
            try:
                payload = {
                    "model": model,
                    "messages": [
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": current_prompt},
                    ],
                    "stream": False,
                    "format": "json",
                    "options": {"temperature": 0.2},
                }
                async with session.post(f"{ollama_url}/api/chat", json=payload) as resp:
                    if resp.status != 200:
                        raise Exception(f"API Error {resp.status}")
                    data = await resp.json()
                    response_text = data["message"]["content"]

                try:
                    parsed_data = json.loads(response_text)
                except json.JSONDecodeError:
                    current_prompt = (
                        base_user_prompt + "\n\nğŸš¨ JSON í˜•ì‹ì´ ì˜ëª»ë˜ì—ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‘ì„±í•˜ì„¸ìš”."
                    )
                    continue

                if guardrail:
                    check = await guardrail.check(parsed_data.get("summary", ""), keyword=keyword)
                    if check.action == GuardrailAction.REJECT:
                        current_prompt = (
                            base_user_prompt
                            + f"\n\nğŸš¨ ì•ˆì „ì„± ìœ„ë°°: {check.issue_summary}. ìˆ˜ì •í•˜ì„¸ìš”."
                        )
                        continue
                    elif check.action == GuardrailAction.REVISE:
                        parsed_data["summary"] = check.revised_content

                final_analysis = parsed_data
                break
            except Exception as e:
                if attempt == max_retries:
                    print(f"   âš ï¸ ë¶„ì„ ì‹¤íŒ¨: {e}")

    duration = time.time() - start

    if final_analysis:
        if console:
            s = final_analysis.get("sentiment", {})
            pos, neg, neu = s.get("positive", 0), s.get("negative", 0), s.get("neutral", 0)
            console.print(
                Panel(
                    f"[bold]ğŸ“Œ í•µì‹¬ ì›ì¸[/]\n{final_analysis.get('main_cause', '-')}\n\n"
                    f"[bold]ğŸ“ ìš”ì•½[/]\n{final_analysis.get('summary', '-')}\n\n"
                    f"[bold]ğŸ“Š ê°ì„± ë¶„í¬[/]\nğŸ˜Š {pos:.0%} | ğŸ˜  {neg:.0%} | ğŸ˜ {neu:.0%}\n\n"
                    f"[bold]ğŸ’¡ í•µì‹¬ í¬ì¸íŠ¸[/]\n"
                    + "\n".join(f"â€¢ {op}" for op in final_analysis.get("key_opinions", [])[:3]),
                    title=f"ğŸ“Š ë¶„ì„ ê²°ê³¼: {keyword}",
                    border_style="green",
                )
            )

    return final_analysis, {"duration": duration}


# =============================================================================
# Main Pipeline (ìˆ˜ì •ëœ í•µì‹¬ ë¡œì§)
# =============================================================================
async def run_real_pipeline(
    keywords: list[str] | None = None,
    max_keywords: int = 10,
    max_articles: int = 15,
    ollama_url: str = os.getenv("OLLAMA_HOST", "http://localhost:11434"),
    model: str = "exaone3.5",
    output_dir: Path = Path("./output/images"),
) -> dict:
    # 1. Trigger
    trend_keywords_data = await stage_trigger(max_keywords)
    if not trend_keywords_data:
        return {"success": False}

    # 2. Collection (ì „ì²´ í‚¤ì›Œë“œ ìˆ˜ì§‘)
    articles = await stage_collection(trend_keywords_data, max_articles)
    if not articles:
        return {"success": False}

    # 3. Deduplication & Indexing
    await stage_deduplication_and_indexing(articles)

    # 4. Analysis Loop (ì „ì²´ í‚¤ì›Œë“œ ë°˜ë³µ ë¶„ì„)
    print_stage(f"Stage 4: LLM ANALYSIS - ì „ì²´ ë¶„ì„ ({len(trend_keywords_data)}ê°œ í† í”½)")

    analysis_results = []

    for idx, kw_data in enumerate(trend_keywords_data, 1):
        target_keyword = kw_data["keyword"]
        print(f"\n[Topic {idx}/{len(trend_keywords_data)}] Analyzing: '{target_keyword}'...")

        # [ìˆ˜ì • 2] DB ì¤‘ë³µ ì—¬ë¶€ ìƒê´€ì—†ì´ 'í˜„ì¬ ìˆ˜ì§‘ëœ ê¸°ì‚¬' ì‚¬ìš© (ë¦¬í¬íŠ¸ ìƒì„± ë³´ì¥)
        target_articles = [art for art in articles if art.get("keyword") == target_keyword]

        # ê¸°ì‚¬ ë‚´ ë‹¨ìˆœ ì¤‘ë³µ ì œê±° (ì œëª© ê¸°ì¤€)
        seen = set()
        unique_target_articles = []
        for art in target_articles:
            if art["title"] not in seen:
                seen.add(art["title"])
                unique_target_articles.append(art)

        if not unique_target_articles:
            print(f"   âš ï¸ ìˆ˜ì§‘ëœ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤: '{target_keyword}' (Skipping)")
            continue

        # RAG ê²€ìƒ‰
        context_docs = await stage_rag_search(target_keyword, unique_target_articles)

        # ë°ì´í„° ë³‘í•© (ê³¼ê±° ë¬¸ë§¥ + í˜„ì¬ ê¸°ì‚¬)
        rag_augmented_articles = []
        for ctx in context_docs:
            rag_augmented_articles.append(
                {
                    "title": f"[ê³¼ê±° ë¬¸ë§¥] {ctx.get('date', '')[:10]}",
                    "summary": ctx["content"],
                    "source": "TrendOps Memory",
                    "published": ctx.get("date", ""),
                    "keyword": target_keyword,
                }
            )
        rag_augmented_articles.extend(unique_target_articles)

        # LLM ë¶„ì„
        analysis_result, _ = await stage_llm_analysis(
            keyword=target_keyword,
            articles=rag_augmented_articles,
            ollama_url=ollama_url,
            model=model,
        )

        if analysis_result:
            analysis_results.append(analysis_result)
            # [ì¶”ê°€ë¨] ë¦¬í¬íŠ¸ ì €ì¥
            if report_service:
                report_service.save_analysis(target_keyword, analysis_result)
                print("   ğŸ’¾ ë¦¬í¬íŠ¸ ë°ì´í„° ì €ì¥ ì™„ë£Œ")

        await asyncio.sleep(1)

    if console:
        console.print(Panel("ğŸ‰ ì „ì²´ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!", style="bold green"))

    return {"success": True, "analysis_count": len(analysis_results)}


if __name__ == "__main__":
    asyncio.run(run_real_pipeline())
