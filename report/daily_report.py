import json
from pathlib import Path
from datetime import datetime, timedelta
from jinja2 import Environment, FileSystemLoader
import logging

logger = logging.getLogger("daily_report")

class DailyReportGenerator:
    def __init__(self, template_dir: str = "src/trendops/report/templates"):
        self.template_dir = Path(template_dir)
        # Jinja2 í™˜ê²½ ì„¤ì •
        self.env = Environment(loader=FileSystemLoader(self.template_dir))

    def generate(self, log_file: Path, output_file: Path) -> str | None:
        """ë¡œê·¸ íŒŒì¼ì„ ì½ì–´ ë¦¬í¬íŠ¸ ìƒì„±"""
        if not log_file.exists():
            logger.warning("No log file found.")
            return None

        # 1. ë°ì´í„° ë¡œë”© ë° í†µê³„ ê³„ì‚°
        data = self._process_logs(log_file)
        
        # 2. í…œí”Œë¦¿ ë Œë”ë§
        try:
            template = self.env.get_template("daily_report.md.j2")
            rendered_report = template.render(**data)
            
            # 3. íŒŒì¼ ì €ì¥
            with open(output_file, "w", encoding="utf-8") as f:
                f.write(rendered_report)
            
            return str(output_file)
        except Exception as e:
            logger.error(f"Failed to render report: {e}")
            return None

    def _process_logs(self, log_file: Path) -> dict:
        """JSONL ë¡œê·¸ë¥¼ íŒŒì‹±í•˜ì—¬ í†µê³„ ë°ì´í„° ìƒì„±"""
        entries = []
        with open(log_file, "r", encoding="utf-8") as f:
            for line in f:
                if line.strip():
                    entries.append(json.loads(line))

        # --- í†µê³„ ê³„ì‚° ë¡œì§ (Mockup í¬í•¨) ---
        
        # 1. Top 5 íŠ¸ë Œë“œ (ìµœì‹ ìˆœ 5ê°œ ì¶”ì¶œ)
        top_trends = []
        for entry in entries[-5:]: # ë’¤ì—ì„œ 5ê°œ
            an = entry["analysis"]
            
            # ê°ì„± í¬ë§·íŒ…
            pos = an.get("sentiment", {}).get("positive", 0)
            neg = an.get("sentiment", {}).get("negative", 0)
            if pos > 0.5: sent = f"ğŸŸ¢ ê¸ì • {int(pos*100)}%"
            elif neg > 0.5: sent = f"ğŸ”´ ë¶€ì • {int(neg*100)}%"
            else: sent = f"âšª ì¤‘ë¦½ {int((1-pos-neg)*100)}%"

            top_trends.append({
                "keyword": entry["keyword"],
                "sentiment": sent,
                "cause": an.get("main_cause", "-")[:40] + "..." # ê¸¸ì´ ì œí•œ
            })
        top_trends.reverse() # 1ìœ„ê°€ ìœ„ë¡œ ì˜¤ê²Œ

        # 2. ì‹œìŠ¤í…œ í†µê³„ (ì‹¤ì œ ë°ì´í„° + ì¼ë¶€ Mockup)
        # ì‹¤ì œë¡œëŠ” DBë‚˜ ëª¨ë‹ˆí„°ë§ íˆ´ì—ì„œ ê°€ì ¸ì™€ì•¼ í•˜ì§€ë§Œ, ì§€ê¸ˆì€ ë¡œê·¸ ê°œìˆ˜ë¡œ ì¶”ì •í•©ë‹ˆë‹¤.
        total_collected = sum([an.get("source_count", 0) for an in [e["analysis"] for e in entries]])
        
        stats = {
            "total_collected": total_collected,
            "news_count": int(total_collected * 0.7),   # ì˜ˆì‹œ ë¹„ìœ¨
            "youtube_count": int(total_collected * 0.3),# ì˜ˆì‹œ ë¹„ìœ¨
            "dedup_ratio": 73.2, # (ë‚˜ì¤‘ì— Dedup ëª¨ë“ˆì—ì„œ ë°›ì•„ì™€ì•¼ í•¨)
            "run_count": len(entries),
            "success_rate": 100,
            "avg_duration": 38.2, # (Logì— duration ì¶”ê°€ ì‹œ ê³„ì‚° ê°€ëŠ¥)
            "log_count": len(entries),
            "issues": [] # ì´ìŠˆê°€ ìˆìœ¼ë©´ ì—¬ê¸°ì— ì¶”ê°€
        }

        # 3. ìƒ˜í”Œ ë°ì´í„°
        sample = {"keyword": "-", "summary": "-"}
        if entries:
            last = entries[-1]
            sample = {
                "keyword": last["keyword"],
                "summary": last["analysis"].get("summary", "-")
            }

        # ë‚ ì§œ í¬ë§·íŒ…
        now = datetime.now()
        days = ["ì›”", "í™”", "ìˆ˜", "ëª©", "ê¸ˆ", "í† ", "ì¼"]
        
        return {
            "date": now.strftime("%Y-%m-%d"),
            "day_of_week": days[now.weekday()],
            "top_trends": top_trends,
            "stats": stats,
            "sample": sample,
            "next_run": (now + timedelta(days=1)).strftime("%Y-%m-%d 00:00")
        }